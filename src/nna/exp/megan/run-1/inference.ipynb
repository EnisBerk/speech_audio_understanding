{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/enis/conda/envs/soundenv3/lib/python3.7/site-packages/torchaudio/backend/utils.py:54: UserWarning: \"sox\" backend is being deprecated. The default backend will be changed to \"sox_io\" backend in 0.8.0 and \"sox\" backend will be removed in 0.9.0. Please migrate to \"sox_io\" backend. Please refer to https://github.com/pytorch/audio/issues/903 for the detail.\n",
      "  '\"sox\" backend is being deprecated. '\n"
     ]
    }
   ],
   "source": [
    "# import run\n",
    "# import nna\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "import torchaudio\n",
    "torchaudio.set_audio_backend(\"sox_io\")\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from nna.exp import modelArchs,runutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nna\n",
    "import pandas as pd\n",
    "\n",
    "from nna import visutils\n",
    "from nna import dataimport\n",
    "from nna import fileUtils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class audioDatasetInfer(Dataset):\n",
    "\n",
    "    def __init__(self, X, y=None, transform=None):\n",
    "        '''\n",
    "    Args:\n",
    "\n",
    "    '''\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        #         self.landmarks_frame = pd.read_csv(csv_file)\n",
    "        #         self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.y is None:\n",
    "            sample =  self.X[idx], torch.zeros((2))\n",
    "        else:\n",
    "            sample = self.X[idx], self.y[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample\n",
    "    \n",
    "class ToTensor(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "\n",
    "    def __init__(self, maxMelLen, sampling_rate,device):\n",
    "        # sr = 44100 etc\n",
    "        self.maxMelLen = maxMelLen\n",
    "        self.sampling_rate = sampling_rate\n",
    "        self.device=device\n",
    "        #https://github.com/PCerles/audio/blob/3803d0b27a4e13efa760227ef6c71d0f3753aa98/test/test_transforms.py#L262\n",
    "        #librosa defaults\n",
    "        n_fft = 2048\n",
    "        hop_length = 512\n",
    "        power = 2.0\n",
    "        n_mels = 128\n",
    "        n_mfcc = 40\n",
    "        # htk is false in librosa, no setting in torchaudio -?\n",
    "        # norm is 1 in librosa, no setting in torchaudio -?\n",
    "        self.melspect_transform = torchaudio.transforms.MelSpectrogram(sample_rate=self.sampling_rate, window_fn=torch.hann_window,\n",
    "                                                                  hop_length=hop_length, n_mels=n_mels, n_fft=n_fft).to(device)\n",
    "\n",
    "    \n",
    "        self.db_transform = torchaudio.transforms.AmplitudeToDB(\"power\", 80.).to(device)\n",
    "    def __call__(self, sample):\n",
    "        x, y = sample\n",
    "        x=x.to(self.device)\n",
    "        mel = self.melspect_transform(x.reshape(-1))\n",
    "        an_x = self.db_transform(mel)\n",
    "        #librosa version\n",
    "#         mel = librosa.feature.melspectrogram(y=x.reshape(-1),\n",
    "#                                              sr=self.sampling_rate)\n",
    "#         an_x = librosa.power_to_db(mel, ref=np.max)\n",
    "#         an_x = an_x.astype(\"float32\")\n",
    "#         y = y.astype('float32')\n",
    "#         print(an_x.shape)\n",
    "        an_x = an_x[:, :self.maxMelLen]\n",
    "        # 2-d conv\n",
    "#         x = an_x.reshape(1, *an_x.shape[:])\n",
    "        # 1-d conv\n",
    "        x = an_x.reshape(1, an_x.shape[0]*an_x.shape[1])\n",
    "\n",
    "        \n",
    "        return x,y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def conv_output_shape(h_w, kernel_size=1, stride=1, pad=0, dilation=1):\n",
    "    \"\"\"\n",
    "  Utility function for computing output of convolutions\n",
    "  takes a tuple of (h,w) and returns a tuple of (h,w)\n",
    "  \"\"\"\n",
    "    from math import floor\n",
    "    if type(kernel_size) is not tuple:\n",
    "        kernel_size = (kernel_size, kernel_size)\n",
    "    h = floor(((h_w[0] + (2 * pad) - (dilation *\n",
    "                                      (kernel_size[0] - 1)) - 1) / stride) + 1)\n",
    "    w = floor(((h_w[1] + (2 * pad) - (dilation *\n",
    "                                      (kernel_size[1] - 1)) - 1) / stride) + 1)\n",
    "    return h, w\n",
    "\n",
    "# mel.shape,an_x.shape,X_train.shape\n",
    "class testModel(nn.Module):\n",
    "    '''A simple model for testing by overfitting.\n",
    "    '''\n",
    "    def __init__(self, out_channels, h_w, kernel_size, FLAT=False,output_shape=(10,)):\n",
    "        # h_w: height will be always one since we use 1d convolution \n",
    "        super(testModel, self).__init__()\n",
    "        self.out_channels = out_channels\n",
    "        #### CONV\n",
    "        self.conv1 = nn.Conv1d(in_channels=1, # depth of image == depth of filters\n",
    "                               out_channels=self.out_channels, # number of filters \n",
    "                               kernel_size=kernel_size, # size of the filters/kernels\n",
    "                               padding=1)\n",
    "\n",
    "        self.conv1_shape = conv_output_shape(h_w, kernel_size=kernel_size, stride=1, pad=1, dilation=1)\n",
    "        # conv is 1d\n",
    "        self.conv1_shape = (1,self.conv1_shape[1])\n",
    "        \n",
    "        self.fc1 = nn.Linear(self.out_channels * self.conv1_shape[0] *self.conv1_shape[1], 75)  # 100\n",
    "\n",
    "        self.fc2 = nn.Linear(75,output_shape[0])\n",
    "        \n",
    "        \n",
    "        torch.nn.init.xavier_uniform_(self.conv1.weight)\n",
    "        torch.nn.init.xavier_uniform_(self.fc1.weight)\n",
    "        torch.nn.init.xavier_uniform_(self.fc2.weight)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "#         x = x.reshape(1,)\n",
    "#         print(x.shape) #  50,1,108800 (850*128)\n",
    "        x = F.relu(self.conv1(x))\n",
    "#         x = self.pool(x)\n",
    "        # x = self.drop(x)\n",
    "#         print(x.shape)# 58, 2, 108801\n",
    "#         print(self.conv1_shape)\n",
    "#         print(x.shape)\n",
    "        x = x.view(-1, self.out_channels * self.conv1_shape[0] *self.conv1_shape[1])\n",
    "        # batch_norm is missing\n",
    "        x = F.relu((self.fc1(x)))\n",
    "        x = (self.fc2(x))\n",
    "\n",
    "#         x = self.drop(x)\n",
    "\n",
    "#         x = self.fc4(x)\n",
    "#         x = torch.sigmoid(x)\n",
    "#                 x = F.log_softmax(x,dim=1)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_len = 10\n",
    "\n",
    "def repeat_data(data):\n",
    "    sr = 48000\n",
    "    left_over=(data.shape[0])%(expected_len*sr)\n",
    "    if left_over!=0:\n",
    "        tile_reps = ((expected_len*sr)/(left_over)+1)\n",
    "        # tile_reps\n",
    "        repeated_data = np.tile(data[-left_over:],int(tile_reps))\n",
    "        repeated_data=repeated_data[:expected_len*sr]\n",
    "        return np.concatenate([data[:-left_over],repeated_data])\n",
    "    else:\n",
    "        return data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device=\"cuda:1\"\n",
    "CATEGORY_COUNT=2\n",
    "# '1.1.10','1.1.7'\n",
    "maxMelLen = 938\n",
    "ToTensor_ins = ToTensor(maxMelLen,48000,device)\n",
    "\n",
    "transformCompose = transforms.Compose([\n",
    "    ToTensor_ins,\n",
    "])\n",
    "\n",
    "h_w = [128, 938]\n",
    "kernel_size = (5, 5)\n",
    "\n",
    "\n",
    "output_shape=(CATEGORY_COUNT,)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "testModel(\n",
       "  (conv1): Conv1d(1, 2, kernel_size=(25,), stride=(1,), padding=(1,))\n",
       "  (fc1): Linear(in_features=240084, out_features=75, bias=True)\n",
       "  (fc2): Linear(in_features=75, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path=\"/home/enis/projects/nna/src/nna/exp/megan/run-1/wandb/run-20210114_155758-x6ltobzt/files/best_model_10_ROC_AUC=0.8853.pt\"\n",
    "\n",
    "model_saved = testModel(out_channels=2,h_w=(1,h_w[0]*h_w[1]),kernel_size=kernel_size[0]*kernel_size[0],output_shape=output_shape)\n",
    "model_saved.load_state_dict(torch.load(model_path))\n",
    "model_saved.eval().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #fake data\n",
    "# sample_count=2000\n",
    "# X = np.empty((sample_count*CATEGORY_COUNT,480000),dtype=np.float32)\n",
    "# # y_true = np.random.randint(0,CATEGORY_COUNT,(sample_count))\n",
    "# y_true=[]\n",
    "# for i in range(CATEGORY_COUNT):\n",
    "#     y_true.extend([i]*sample_count)\n",
    "# n_values = np.max(y_true) + 1\n",
    "# y_true = np.eye(n_values)[y_true]\n",
    "\n",
    "# X=torch.from_numpy(X).float()\n",
    "# y_true=torch.from_numpy(y_true).float().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X=torch.from_numpy(X).float()\n",
    "# y_true=torch.from_numpy(y_true).float().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# dataset = {'predict' : audioDatasetInfer(X, y_true, transform=transformCompose)}\n",
    "# dataloader = {'predict': torch.utils.data.DataLoader(dataset['predict'], shuffle=False,batch_size=128)}\n",
    "\n",
    "# outputs=[]\n",
    "# for inputs, labels in dataloader['predict']:\n",
    "#     inputs = inputs.float().to(device)\n",
    "#     output = model_saved(inputs)\n",
    "#     output = output.to(\"cpu\")\n",
    "#     index = output.data.numpy()\n",
    "#     outputs.append(index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "file_properties_df=pd.read_pickle(\"/home/enis/projects/nna/data/realdata_v2No_stinchcomb.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_location = tuple(sorted(set(zip(file_properties_df.region.values,file_properties_df.locationId.values))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_properties_df[file_properties_df.region!='ivvavik'].iloc[0].name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !ls /tank/data/nna/real/prudhoe/15/2019/S4A10283_20190531_164602.flac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_path = \"/tank/data/nna/real/prudhoe/15/2019/S4A10283_20190531_164602.flac\"\n",
    "# dtype = \"\"\n",
    "# sound_array, sr = nna.clippingutils.load_audio(file_path,\n",
    "#                                                dtype=np.int16,\n",
    "#                                                backend='pydub')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sound_array.shape,sr\n",
    "# region_location[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('anwr', '31'),\n",
       " ('anwr', '32'),\n",
       " ('anwr', '33'),\n",
       " ('anwr', '34'),\n",
       " ('anwr', '35'),\n",
       " ('anwr', '36'),\n",
       " ('anwr', '37'),\n",
       " ('anwr', '38'),\n",
       " ('anwr', '39'),\n",
       " ('anwr', '40'),\n",
       " ('anwr', '41'),\n",
       " ('anwr', '42'),\n",
       " ('anwr', '43'),\n",
       " ('anwr', '44'),\n",
       " ('anwr', '45'),\n",
       " ('anwr', '46'),\n",
       " ('anwr', '47'),\n",
       " ('anwr', '48'),\n",
       " ('anwr', '49'),\n",
       " ('anwr', '50'))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 'anwr' only\n",
    "region_location = tuple(sorted(set(zip(file_properties_df.region.values,file_properties_df.locationId.values))))\n",
    "region_location=region_location[:20]\n",
    "region_location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "clipping_results_path= \"/home/enis/projects/nna/src/scripts/clipping_output/\"\n",
    "output_dir='/scratch/enis/data/nna/real/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# with open(megan_labeled_files_info_path, 'r') as f:\n",
    "#     lines = f.readlines()\n",
    "#     lines = [i.strip().split(',') for i in lines]\n",
    "region_location_datasets = {}\n",
    "for region,location in region_location:\n",
    "    \n",
    "    filtered_files = file_properties_df[file_properties_df.region==region]\n",
    "    filtered_files = filtered_files[filtered_files.locationId==location]\n",
    "    dataset_name_v = \"_\".join([region,location])\n",
    "    audio_dataset = dataimport.Dataset(dataset_name_v=dataset_name_v)\n",
    "    for i in filtered_files.iterrows():\n",
    "        audio_dataset[i[0]] = dataimport.Audio(i[1].name, float(i[1].durationSec))\n",
    "    region_location_datasets[(region,location)]=audio_dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anwr 31\n",
      "inference part\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/enis/conda/envs/soundenv3/lib/python3.7/site-packages/torch/functional.py:516: UserWarning: stft will require the return_complex parameter be explicitly  specified in a future PyTorch release. Use return_complex=False  to preserve the current behavior or return_complex=True to return  a complex output. (Triggered internally at  /opt/conda/conda-bld/pytorch_1607370141920/work/aten/src/ATen/native/SpectralOps.cpp:653.)\n",
      "  normalized, onesided, return_complex)\n",
      "/scratch/enis/conda/envs/soundenv3/lib/python3.7/site-packages/torch/functional.py:516: UserWarning: The function torch.rfft is deprecated and will be removed in a future PyTorch release. Use the new torch.fft module functions, instead, by importing torch.fft and calling torch.fft.fft or torch.fft.rfft. (Triggered internally at  /opt/conda/conda-bld/pytorch_1607370141920/work/aten/src/ATen/native/SpectralOps.cpp:590.)\n",
      "  normalized, onesided, return_complex)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anwr 32\n"
     ]
    }
   ],
   "source": [
    "for region,location in region_location:\n",
    "    print(region,location)\n",
    "    region_location_ins=region_location_datasets[(region,location)]\n",
    "    region_location_ins.update_samples_w_clipping_info(output_folder=clipping_results_path)\n",
    "#     print(\"Loading files...\")\n",
    "#     region_location_ins.load_audio_files()\n",
    "#     region_location_ins.pick_channel_by_clipping()\n",
    "\n",
    "    print(\"inference part\")\n",
    "    for audio_ins in region_location_ins.values():\n",
    "        audio_ins.load_data()\n",
    "        audio_ins.pick_channel_by_clipping(expected_len)\n",
    "        input_file_data = repeat_data(audio_ins.data)\n",
    "\n",
    "        # divide to 10 second excerpts\n",
    "        input_file_data = input_file_data.reshape(-1,480000)\n",
    "        input_file_data=torch.from_numpy(input_file_data).float()\n",
    "        dataset = {'predict' : audioDatasetInfer(input_file_data, None, transform=transformCompose)}\n",
    "        dataloader = {'predict': torch.utils.data.DataLoader(dataset['predict'], shuffle=False,batch_size=128)}\n",
    "\n",
    "        outputs=[]\n",
    "        for inputs, labels in dataloader['predict']:\n",
    "            inputs = inputs.float().to(device)\n",
    "            output = model_saved(inputs)\n",
    "            output = output.to(\"cpu\")\n",
    "            index = output.data.numpy()\n",
    "            outputs.append(index)\n",
    "        outputs=np.concatenate(outputs)\n",
    "        row = file_properties_df.loc[audio_ins.path]\n",
    "        for i,n in [(0,'1-1-10'),(1,'1-1-7')]:\n",
    "            sub_directory_addon='V1-'+n\n",
    "            file_name_addon=sub_directory_addon\n",
    "            file_name=fileUtils.standard_path_style(output_dir,row,sub_directory_addon=sub_directory_addon,file_name_addon=file_name_addon)\n",
    "#             print(file_name)\n",
    "            file_name.parent.mkdir(parents=True, exist_ok=True)\n",
    "            np.save(str(file_name)+\".npy\", outputs[:,i])\n",
    "        \n",
    "        audio_ins.data = None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "soundenv3",
   "language": "python",
   "name": "soundenv3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir('/home/enis/projects/nna/src/nna/exp/megan/run-1/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import run\n",
    "import nna\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/enisberk/megan\" target=\"_blank\">https://app.wandb.ai/enisberk/megan</a><br/>\n",
       "                Run page: <a href=\"https://app.wandb.ai/enisberk/megan/runs/1m8yp639\" target=\"_blank\">https://app.wandb.ai/enisberk/megan/runs/1m8yp639</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to query for notebook name, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Wandb version 0.10.12 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "/scratch/enis/data/nna/labeling/megan/AudioSamplesPerSite/meganLabeledFiles_wlenV1.txt\n",
      "/scratch/enis/data/nna/labeling/megan/AudioSamplesPerSite\n",
      "4 files are missing corresponding to excell entries\n",
      "'-> 5 number of samples are DELETED due to ignore_files and missing_audio_files'\n",
      "-> 415 samples DELETED because they are not in the excell\n",
      "\n",
      "-> 0 samples DELETED because they do not have the taxo info coming from excell\n",
      "\n",
      "-> classes that do not have enough data:\n",
      "[REMOVED!]\n",
      "['other-mammal'] 0.0\n",
      "['other-silence'] 20.0\n",
      "['unknown-sound'] 2.0\n",
      "['seabirds'] 1.0\n",
      "['canids'] 1.0\n",
      "['other-flare'] 11.0\n",
      "['other-rain'] 20.0\n",
      "\n",
      "-> classes that have enough data:\n",
      "['other-biophony'] 56.0\n",
      "['other-insect'] 140.0\n",
      "['other-bird'] 661.0\n",
      "['songbirds'] 392.0\n",
      "['duck-goose-swan'] 183.0\n",
      "['grouse-ptarmigan'] 59.0\n",
      "['other-anthrophony'] 66.0\n",
      "['other-aircraft'] 107.0\n",
      "['loons'] 29.0\n",
      "['other-car'] 37.0\n",
      "('-> 102 number of samples are deleted because their taxonomy category does '\n",
      " 'not have enough data.')\n",
      "-> classes that do not have enough data\n",
      "will be REMOVED!\n",
      "-> 97 number of samples are deleted because their length is not long enough.\n"
     ]
    }
   ],
   "source": [
    "audio_dataset,_ = prepare_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file_path = '/scratch/enis/data/nna/labeling/megan/AudioSamplesPerSite/files_as_np_filtered_v3_int16.pkl'\n",
    "# audio_dataset.load_audio_files(cached_dict_path=output_file_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#performance tests of loading files\n",
    "# %%time # 75 seconds\n",
    "# audio_dataset.load_audio_files()\n",
    "# %%time # 42 second\n",
    "# audio_dataset.load_audio_files(cached_dict_path=output_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # save files # first time\n",
    "# audio_dataset.load_audio_files()\n",
    "# audio_dataset.pick_channel_by_clipping()\n",
    "# audio_dataset.create_cache_pkl(output_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just load\n",
    "audio_dataset.load_audio_files(output_file_path)\n",
    "audio_dataset.pick_channel_by_clipping()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1045414,)\n"
     ]
    }
   ],
   "source": [
    "for key, value in audio_dataset.items():\n",
    "    # data = cached_dict.get(str(value.path), None)\n",
    "    print(value.data.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/scratch/enis/data/nna/labeling/megan/AudioSamplesPerSite/Site_11/SP1/S4A10276_20190710_091602_Bio_01.wav'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(value.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pathlib.PosixPath'> /scratch/enis/data/nna/labeling/megan/AudioSamplesPerSite/Site_11/SP1/S4A10276_20190710_091602_Bio_01.wav (1045414,)\n"
     ]
    }
   ],
   "source": [
    "for k, v in cached_dict.items():\n",
    "    print(type(k),k,v[0].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "data_file = {}\n",
    "if Path(output_file_path).exists():\n",
    "    raise ValueError(f'{output_file_path} already exists')\n",
    "for k, v in audio_dataset.items():\n",
    "    data_file[k] = v.data[:], v.sr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "d=iter(data_file.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-4073., -4064., -4047., ..., -7174., -7237., -7305.]), 48000)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "for v in data_file.values():\n",
    "    if len(v[0].shape)>1:\n",
    "        print(v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(output_file_path, 'wb') as f:\n",
    "    np.save(f, data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "b=iter(audio_dataset.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.        , 0.00509167],\n",
       "        [0.00678958, 0.02112917],\n",
       "        [0.        , 0.0131126 ]]),\n",
       " 21.779458,\n",
       " (2, 1045414))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=next(b)\n",
    "a.clipping,a.length,a.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.        , 0.00509167],\n",
       "        [0.00678958, 0.02112917],\n",
       "        [0.        , 0.0131126 ]]),\n",
       " 21.779458,\n",
       " (1045414,))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=next(b)\n",
    "a.clipping,a.length,a.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.00106042, 0.00228542],\n",
       "        [0.        , 0.        ]]),\n",
       " 12.391771,\n",
       " (594805,))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=next(b)\n",
    "a.clipping,a.length,a.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.        , 0.00509167],\n",
       "        [0.00678958, 0.02112917],\n",
       "        [0.        , 0.0131126 ]]),\n",
       " 21.779458,\n",
       " (2, 1045414))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=next(b)\n",
    "a.clipping,a.length,a.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pydub import AudioSegment\n",
    "# import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\n",
    "    f\"cuda:{config.device}\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# labelsbyhumanpath = Path('/scratch/enis/data/nna/labeling/results/')\n",
    "# sourcePath = Path(\"/scratch/enis/data/nna/labeling/splits/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    " # RAW DATA\n",
    " def load_raw_data(labelsbyhumanpath,sourcePath):\n",
    "    # load labels for data\n",
    "    with open(labelsbyhumanpath/\"np_array_Ymatrix.npy\", 'rb') as f:\n",
    "        y_true = np.load(f)\n",
    "    \n",
    "        # ## load Dataset\n",
    "        # # X.shape is (1300, 10, 44100)\n",
    "    with open(sourcePath/\"np_array_Xmatrix_shortby441000.npy\", 'rb') as f:\n",
    "        X = np.load(f)\n",
    "        #\n",
    "        # ### split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "                    X, y_true, test_size=0.2, random_state=42)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "                    X_train, y_train, test_size=0.25, random_state=42)\n",
    "    return X_train,X_test,X_val,y_train,y_test,y_val\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,X_val,y_train,y_test,y_val = load_raw_data(labelsbyhumanpath,sourcePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(780, 10, 44100)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # RAW DATA\n",
    "    # load labels for data\n",
    "    # with open(labelsbyhumanpath/\"np_array_Ymatrix.npy\", 'rb') as f:\n",
    "    #     y_true = np.load(f)\n",
    "    #\n",
    "    #     # ## load Dataset\n",
    "    #     # # X.shape is (1300, 10, 44100)\n",
    "    # with open(sourcePath/\"np_array_Xmatrix_shortby441000.npy\", 'rb') as f:\n",
    "    #     X = np.load(f)\n",
    "    #     #\n",
    "    #     # ### split data\n",
    "    # X_train, X_test, y_train, y_test = train_test_split(\n",
    "    #                 X, y_true, test_size=0.2, random_state=42)\n",
    "    # X_train, X_val, y_train, y_val = train_test_split(\n",
    "    #                 X_train, y_train, test_size=0.25, random_state=42)\n",
    "    #\n",
    "\n",
    "    ##### AUGMENTATIONS #######\n",
    "\n",
    "    # ###### randomADD\n",
    "    # X_trainAugmented,y_trainAugmented = augmentations.randomAdd(X_train,y_train,config[\"augmentadSize\"],unique=False)\n",
    "    # X_train = X_trainAugmented\n",
    "    # y_train = y_trainAugmented\n",
    "    # # to keep original ones in the same shape\n",
    "    # # increase size of original ones by merging them with themselves or we could just use augmentad ones\n",
    "    #\n",
    "    # ########### calculate mel-spectogram for all data\n",
    "    #\n",
    "    # XArrays=[X_val,X_test,X_train]\n",
    "    # X_val,X_test,X_train=runUtils.clipped_mel_loop(XArrays,850)\n",
    "    #\n",
    "    #     # # add channel dimension and turn data into float32\n",
    "    # XArrays=[X_train,X_test,X_val]\n",
    "    # for i,X_array in enumerate(XArrays):\n",
    "    #     XArrays[i]=X_array.reshape(X_array.shape[0],1,*X_array.shape[1:])\n",
    "    #\n",
    "    # [X_train,X_test,X_val]=XArrays\n",
    "\n",
    "    # AUGMENTED ONES\n",
    "    # with open(sourcePath/\"np_array_Xmatrix_addAug_mel.npy\", 'rb') as f:\n",
    "    #     XArrays = pickle.load(f)\n",
    "    #     [X_train,X_test,X_val]=XArrays\n",
    "    # with open(sourcePath/\"np_array_Ymatrix_seperate.npy\", 'rb') as f:\n",
    "    #     yArrays = pickle.load(f)\n",
    "    #     [y_train,y_test,y_val]=yArrays"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "speechEnv",
   "language": "python",
   "name": "speechenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

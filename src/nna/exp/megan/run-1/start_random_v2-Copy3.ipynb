{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir('/home/enis/projects/nna/src/nna/exp/megan/run-1/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/enis/conda/envs/soundenv3/lib/python3.7/site-packages/torchaudio/backend/utils.py:54: UserWarning: \"sox\" backend is being deprecated. The default backend will be changed to \"sox_io\" backend in 0.8.0 and \"sox\" backend will be removed in 0.9.0. Please migrate to \"sox_io\" backend. Please refer to https://github.com/pytorch/audio/issues/903 for the detail.\n",
      "  '\"sox\" backend is being deprecated. '\n"
     ]
    }
   ],
   "source": [
    "# import run\n",
    "# import nna\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "import torchaudio\n",
    "torchaudio.set_audio_backend(\"sox_io\")\n",
    "\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(94)\n",
    "\n",
    "np.random.seed(94)\n",
    "\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_solution_per_taxo_w_loc_name={'1.0.0': [['44', '46', '17', '14'],\n",
    "  ['11', '34', '27'],\n",
    "  ['31', '50', '18', '12', '30', '39', '48', '45']],\n",
    " '3.0.0': [['40', '20', '14', '17', '13', '36', '25', '33'],\n",
    "  ['18', '38', '39'],\n",
    "  ['32', '45']],\n",
    " 'X.X.X': [['45', '14', '27', '25', '34', '46', '29', '18', '38'],\n",
    "  ['36'],\n",
    "  ['32', '20', '21']],\n",
    " '1.1.10': [['49', '48', '19', '16', '22', '37', '29', '25', '31', '27'],\n",
    "  ['46', '20', '11', '33', '24'],\n",
    "  ['17', '21', '39', '30', '38', '18', '47', '50', '14']],\n",
    " '1.1.0': [['12',\n",
    "   '37',\n",
    "   '11',\n",
    "   '22',\n",
    "   '18',\n",
    "   '44',\n",
    "   '29',\n",
    "   '46',\n",
    "   '13',\n",
    "   '34',\n",
    "   '25',\n",
    "   '24',\n",
    "   '17',\n",
    "   '40',\n",
    "   '31',\n",
    "   '27',\n",
    "   '14'],\n",
    "  ['19', '16', '39', '30', '38', '41'],\n",
    "  ['50', '20', '47', '49', '48', '21', '15', '36']],\n",
    " '1.3.0': [['21', '40', '32', '39', '41', '44'],\n",
    "  ['50', '20', '38', '11'],\n",
    "  ['24', '19', '27', '14']],\n",
    " '1.1.8': [['20', '16', '11', '37', '15', '25', '31'],\n",
    "  ['21', '49', '38'],\n",
    "  ['22', '40']],\n",
    " '1.1.7': [['16', '30', '15', '46', '27'],\n",
    "  ['49', '25', '24'],\n",
    "  ['20', '39', '48', '19', '38', '22', '29', '11', '40']],\n",
    " '0.2.0': [['17'], ['24', '27'], ['19', '15']],\n",
    " '0.0.0': [['29'], ['24'], ['12', '27']],\n",
    " '1.1.3': [['46', '25'], ['24'], ['27', '22', '38']],\n",
    " '2.1.0': [['34', '33'], ['36'], ['46']],\n",
    " '0.1.0': [['29'], ['25'], ['24']]}\n",
    "error_because_small = ['1.2.4',\n",
    "'1.1.5',\n",
    "'1.2.0',\n",
    "'0.4.0']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import runconfigs\n",
    "import wandb\n",
    "from ignite.metrics import Accuracy, Loss\n",
    "from ignite.contrib.metrics import ROC_AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nna.exp import augmentations,\n",
    "from nna.exp import modelArchs,runutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandb.init(config=runconfigs.default_config, project=runconfigs.PROJECT_NAME)\n",
    "# config = wandb.config\n",
    "config = runconfigs.default_config\n",
    "# wandb.config.update(args) # adds all of the arguments as config variables\n",
    "\n",
    "params = {\n",
    "    'batch_size': config['batch_size'],\n",
    "    'shuffle': True,\n",
    "    'num_workers': 0\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\n",
    "    f\"cuda:{config['device']}\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# labelsbyhumanpath = Path('/scratch/enis/data/nna/labeling/results/')\n",
    "# sourcePath = Path(\"/scratch/enis/data/nna/labeling/splits/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "CATEGORY_COUNT = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    " # RAW DATA\n",
    "def load_raw_data(sample_count,CATEGORY_COUNT):\n",
    "    sample_count = 1200\n",
    "    X = np.empty((sample_count,480000),dtype=np.float32)\n",
    "    y_true = np.random.randint(0,CATEGORY_COUNT,(sample_count))\n",
    "    for i,y in enumerate(y_true):\n",
    "        X[i,:] = y\n",
    "    X = np.interp(X, (X.min(), X.max()), (-32768 , 32767))\n",
    "    n_values = np.max(y_true) + 1\n",
    "    y_true = np.eye(n_values)[y_true]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "                    X, y_true, test_size=0.2, random_state=42)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "                    X_train, y_train, test_size=0.25,random_state=42)\n",
    "    return X_train,X_test,X_val,y_train,y_test,y_val\n",
    "\n",
    "def mock_raw_data(sample_count,CATEGORY_COUNT):\n",
    "#     sample_count = 30\n",
    "    X = np.empty((sample_count*CATEGORY_COUNT,480000),dtype=np.float32)\n",
    "    # y_true = np.random.randint(0,CATEGORY_COUNT,(sample_count))\n",
    "    y_true=[]\n",
    "    for i in range(CATEGORY_COUNT):\n",
    "        y_true.extend([i]*sample_count)\n",
    "    print(len(y_true))\n",
    "    for i,y in enumerate(y_true):\n",
    "        X[i,:] = y\n",
    "    X = np.interp(X, (X.min(), X.max()), (0 , 3400))\n",
    "#     X = np.interp(X, (X.min(), X.max()), (-32768 , 32767))\n",
    "    n_values = np.max(y_true) + 1\n",
    "    y_true = np.eye(n_values)[y_true]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "                    X, y_true, test_size=0.2, random_state=42)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "                    X_train, y_train, test_size=0.25,random_state=42)\n",
    "#     print(y_train)\n",
    "    return X_train,X_test,X_val,y_train,y_test,y_val\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/scratch/enis/data/nna/labeling/megan/AudioSamplesPerSite/meganLabeledFiles_wlenV1.txt\n",
      "/scratch/enis/data/nna/labeling/megan/AudioSamplesPerSite\n",
      "4 files are missing corresponding to excell entries\n",
      "'-> 5 number of samples are DELETED due to ignore_files and missing_audio_files'\n",
      "-> 415 samples DELETED because they are not in the excell\n",
      "\n",
      "-> 0 samples DELETED because they do not have the taxo info coming from excell\n",
      "\n",
      "-> classes that do not have enough data:\n",
      "[REMOVED!]\n",
      "['other-mammal'] 0.0\n",
      "['other-silence'] 20.0\n",
      "['unknown-sound'] 2.0\n",
      "['seabirds'] 1.0\n",
      "['canids'] 1.0\n",
      "['other-flare'] 11.0\n",
      "['other-rain'] 20.0\n",
      "\n",
      "-> classes that have enough data:\n",
      "['other-biophony'] 56.0\n",
      "['other-insect'] 140.0\n",
      "['other-bird'] 661.0\n",
      "['songbirds'] 392.0\n",
      "['duck-goose-swan'] 183.0\n",
      "['grouse-ptarmigan'] 59.0\n",
      "['other-anthrophony'] 66.0\n",
      "['other-aircraft'] 107.0\n",
      "['loons'] 29.0\n",
      "['other-car'] 37.0\n",
      "('-> 102 number of samples are deleted because their taxonomy category does '\n",
      " 'not have enough data.')\n",
      "-> classes that do not have enough data\n",
      "will be REMOVED!\n",
      "-> 97 number of samples are deleted because their length is not long enough.\n",
      "loading from cache at /scratch/enis/data/nna/labeling/megan/AudioSamplesPerSite/files_as_np_filtered_v3_int16.pkl\n"
     ]
    }
   ],
   "source": [
    "## Load real data rather than mock \n",
    "    # MVP1: delete parts longer than 10 seconds\n",
    "import run\n",
    "audio_dataset,_ = run.prepare_dataset()\n",
    "\n",
    "output_file_path = '/scratch/enis/data/nna/labeling/megan/AudioSamplesPerSite/files_as_np_filtered_v3_int16.pkl'\n",
    "audio_dataset.load_audio_files(output_file_path)\n",
    "audio_dataset.pick_channel_by_clipping()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sound_ins=next(iter(audio_dataset.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'11'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sound_ins[1].location_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(589, 589)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sound_ins[1].taxo_code\n",
    "# classA = 1.1.7 #'duck-goose-swan']\n",
    "# classB = 0.2.0 # other-aircraft\n",
    "sampleTest= []\n",
    "y=[]\n",
    "location_id_info = []\n",
    "expected_len=10\n",
    "for sound_ins in audio_dataset.values():\n",
    "    if sound_ins.taxo_code in ['1.1.10','1.1.7']:\n",
    "        y.append(sound_ins.taxo_code)\n",
    "        location_id_info.append(sound_ins.location_id)\n",
    "        if sound_ins.length<10:\n",
    "            tile_reps = (expected_len/(sound_ins.length)+1)\n",
    "            repeated_data = np.tile(sound_ins.data,int(tile_reps))\n",
    "            repeated_data = repeated_data[:expected_len*sound_ins.sr]\n",
    "            sampleTest.append(repeated_data)\n",
    "        else:\n",
    "            sampleTest.append(sound_ins.data[:expected_len*sound_ins.sr])\n",
    "\n",
    "len(sampleTest),len(y)\n",
    "\n",
    "# sampleTest=np.array(sampleTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import argmax\n",
    "# define input string\n",
    "# define universe of possible input values\n",
    "alphabet = ['1.1.10','1.1.7']\n",
    "# define a mapping of chars to integers\n",
    "char_to_int = dict((c, i) for i, c in enumerate(alphabet))\n",
    "int_to_char = dict((i, c) for i, c in enumerate(alphabet))\n",
    "# integer encode input data\n",
    "integer_encoded = [char_to_int[char] for char in y]\n",
    "# print(integer_encoded)\n",
    "# one hot encode\n",
    "onehot_encoded = list()\n",
    "for value in integer_encoded:\n",
    "\tletter = [0 for _ in range(len(alphabet))]\n",
    "\tletter[value] = 1\n",
    "\tonehot_encoded.append(letter)\n",
    "# print(onehot_encoded)\n",
    "# invert encoding\n",
    "inverted = int_to_char[argmax(onehot_encoded[0])]\n",
    "# print(inverted)\n",
    "onehot_encoded=np.array(onehot_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test, X_val, y_train, y_test,y_val  = [],[],[],[],[],[]\n",
    "for sample,taxo,y_val_ins,loc_id in  zip(sampleTest,y,onehot_encoded,location_id_info):\n",
    "    if loc_id in best_solution_per_taxo_w_loc_name[taxo][0]:\n",
    "        X_train.append(sample)\n",
    "        y_train.append(y_val_ins)\n",
    "    elif loc_id in best_solution_per_taxo_w_loc_name[taxo][1]:\n",
    "        X_test.append(sample)\n",
    "        y_test.append(y_val_ins)\n",
    "    elif loc_id in best_solution_per_taxo_w_loc_name[taxo][2]:\n",
    "        X_val.append(sample)\n",
    "        y_val.append(y_val_ins)\n",
    "    else:\n",
    "        print('error')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(589, 589)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sampleTest),361+113+115"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,X_val=np.array(X_train),np.array(X_test),np.array(X_val)\n",
    "y_train,y_test,y_val=np.array(y_train),np.array(y_test),np.array(y_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_test, y_train, y_test = train_test_split(\n",
    "#                 sampleTest, onehot_encoded, test_size=0.2, random_state=42)\n",
    "# X_train, X_val, y_train, y_val = train_test_split(\n",
    "#                 X_train, y_train, test_size=0.25,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,X_val=torch.from_numpy(X_train).float(),torch.from_numpy(X_test).float(),torch.from_numpy(X_val).float()\n",
    "y_train,y_test,y_val=torch.from_numpy(y_train).float(),torch.from_numpy(y_test).float(),torch.from_numpy(y_val).float()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([361, 480000]),\n",
       " torch.Size([113, 480000]),\n",
       " torch.Size([115, 480000]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape,X_test.shape,X_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([361, 2]), torch.Size([113, 2]), torch.Size([115, 2]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape,y_test.shape,y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1.],\n",
       "        [0., 1.],\n",
       "        [1., 0.]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # X_train,X_test,X_val,y_train,y_test,y_val = load_raw_data('labelsbyhumanpath','sourcePath')\n",
    "# X_train,X_test,X_val,y_train,y_test,y_val = mock_raw_data(4,CATEGORY_COUNT)\n",
    "# X_train,X_test,X_val=torch.from_numpy(X_train).float(),torch.from_numpy(X_test).float(),torch.from_numpy(X_val).float()\n",
    "# y_train,y_test,y_val=torch.from_numpy(y_train).float(),torch.from_numpy(y_test).float(),torch.from_numpy(y_val).float()\n",
    "\n",
    "# # labelsbyhumanpath = Path('/scratch/enis/data/nna/labeling/results/')\n",
    "# # with open(labelsbyhumanpath/\"np_array_Ymatrix.npy\", 'rb') as f:\n",
    "# #     y_true = np.load(f)\n",
    "\n",
    "# Counter(np.argmax(y_train,axis=1).tolist()),Counter(np.argmax(y_val,axis=1).tolist()),Counter(np.argmax(y_test,axis=1).tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# X_train2=np.interp(X_train, (X_train.min(), X_train.max()), (-32768 , 32767))\n",
    "# torch.from_numpy(X_train2).float()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# smaple_index=10\n",
    "# X_train[smaple_index,:],y_train[smaple_index,]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train[0,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multiply(torch.ones((1,2)),torch.ones((1,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToTensor(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "\n",
    "    def __init__(self, maxMelLen, sampling_rate):\n",
    "        # sr = 44100 etc\n",
    "        self.maxMelLen = maxMelLen\n",
    "        self.sampling_rate = sampling_rate\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        x, y = sample\n",
    "        #https://github.com/PCerles/audio/blob/3803d0b27a4e13efa760227ef6c71d0f3753aa98/test/test_transforms.py#L262\n",
    "        #librosa defaults\n",
    "        n_fft = 2048\n",
    "        hop_length = 512\n",
    "        power = 2.0\n",
    "        n_mels = 128\n",
    "        n_mfcc = 40\n",
    "        # htk is false in librosa, no setting in torchaudio -?\n",
    "        # norm is 1 in librosa, no setting in torchaudio -?\n",
    "        melspect_transform = torchaudio.transforms.MelSpectrogram(sample_rate=self.sampling_rate, window_fn=torch.hann_window,\n",
    "                                                                  hop_length=hop_length, n_mels=n_mels, n_fft=n_fft)\n",
    "\n",
    "    \n",
    "        db_transform = torchaudio.transforms.AmplitudeToDB(\"power\", 80.)\n",
    "        mel = melspect_transform(x.reshape(-1))\n",
    "        an_x = db_transform(mel)\n",
    "        #librosa version\n",
    "#         mel = librosa.feature.melspectrogram(y=x.reshape(-1),\n",
    "#                                              sr=self.sampling_rate)\n",
    "#         an_x = librosa.power_to_db(mel, ref=np.max)\n",
    "#         an_x = an_x.astype(\"float32\")\n",
    "#         y = y.astype('float32')\n",
    "#         print(an_x.shape)\n",
    "        an_x = an_x[:, :self.maxMelLen]\n",
    "        # 2-d conv\n",
    "#         x = an_x.reshape(1, *an_x.shape[:])\n",
    "        # 1-d conv\n",
    "        x = an_x.reshape(1, an_x.shape[0]*an_x.shape[1])\n",
    "\n",
    "        \n",
    "        return x,y\n",
    "\n",
    "# #test\n",
    "# maxMelLen_test = 850\n",
    "# SAMPLING_RATE_test = 48000\n",
    "# sample_len_seconds = 10\n",
    "# # to_tensor works on single sample\n",
    "# sample_count = 1\n",
    "# xx_test = torch.ones((sample_count,SAMPLING_RATE_test*sample_len_seconds))\n",
    "# y_values = torch.ones(sample_count)\n",
    "# \n",
    "# toTensor = ToTensor(maxMelLen_test,SAMPLING_RATE_test)\n",
    "# x_out,y_out=toTensor((xx_test,y_values))\n",
    "# x_out.shape,y_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([361, 480000])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X_train,X_test,X_val,y_train,y_test,y_val\n",
    "X_train[:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# toTensor = ToTensor(maxMelLen_test,SAMPLING_RATE_test)\n",
    "# x_out2,y_out=toTensor((X_train[1:2,:],y_train))\n",
    "# x_out.shape,y_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.mean(x_out[0]),torch.mean(x_out2[0])\n",
    "# y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train,y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# pitch = augmentations.pitch_shift_n_stepsClass(\n",
    "#     runconfigs.SAMPLING_RATE, config['pitch_shift_n_steps'])\n",
    "# noise = augmentations.addNoiseClass(config['noise_factor'])\n",
    "# strech = augmentations.time_stretchClass(runconfigs.SAMPLING_RATE*runconfigs.EXCERPT_LENGTH,\n",
    "#                                             config['time_stretch_factor'],\n",
    "#                                             isRandom=True)\n",
    "# shift = augmentations.shiftClass(config['roll_rate'], isRandom=True)\n",
    "maxMelLen = 938 # old 850\n",
    "# toTensor = augmentations.ToTensor(maxMelLen,runconfigs.SAMPLING_RATE)\n",
    "toTensor = ToTensor(maxMelLen,runconfigs.SAMPLING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ignite.metrics import EpochMetric\n",
    "\n",
    "\n",
    "def roc_auc_perClass_compute_fn(y_preds, y_targets):\n",
    "    try:\n",
    "        from sklearn.metrics import roc_auc_score\n",
    "    except ImportError:\n",
    "        raise RuntimeError(\n",
    "            \"This contrib module requires sklearn to be installed.\")\n",
    "\n",
    "    y_true = y_targets.numpy()\n",
    "    y_pred = y_preds.numpy()\n",
    "#     print(y_pred,y_true)\n",
    "#     res = []\n",
    "#     for y_true_perClass_Index in y_true.shape[1]:\n",
    "#         res.append(\n",
    "#             roc_auc_score(y_true[:, y_true_perClass_Index],\n",
    "#                           y_pred[:, y_true_perClass_Index]))\n",
    "    res = roc_auc_score(y_true, y_pred, average=None)\n",
    "    return res\n",
    "\n",
    "\n",
    "#[docs]\n",
    "class ROC_AUC_perClass(EpochMetric):\n",
    "    \"\"\"Computes Area Under the Receiver Operating Characteristic Curve (ROC AUC)\n",
    "  accumulating predictions and the ground-truth during an epoch and applying\n",
    "  `sklearn.metrics.roc_auc_score <http://scikit-learn.org/stable/modules/generated/\n",
    "  sklearn.metrics.roc_auc_score.html#sklearn.metrics.roc_auc_score>`_ .\n",
    "\n",
    "  Args:\n",
    "      output_transform (callable, optional): a callable that is used to transform the\n",
    "          :class:`~ignite.engine.engine.Engine`'s ``process_function``'s output into the\n",
    "          form expected by the metric. This can be useful if, for example, you have a multi-output model and\n",
    "          you want to compute the metric with respect to one of the outputs.\n",
    "      check_compute_fn (bool): Optional default False. If True, `roc_curve\n",
    "          <http://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html#\n",
    "          sklearn.metrics.roc_auc_score>`_ is run on the first batch of data to ensure there are\n",
    "          no issues. User will be warned in case there are any issues computing the function.\n",
    "\n",
    "  ROC_AUC expects y to be comprised of 0's and 1's. y_pred must either be probability estimates or confidence\n",
    "  values. To apply an activation to y_pred, use output_transform as shown below:\n",
    "\n",
    "  .. code-block:: python\n",
    "\n",
    "      def activated_output_transform(output):\n",
    "          y_pred, y = output\n",
    "          y_pred = torch.sigmoid(y_pred)\n",
    "          return y_pred, y\n",
    "\n",
    "      roc_auc = ROC_AUC(activated_output_transform)\n",
    "\n",
    "  \"\"\"\n",
    "    def __init__(self,\n",
    "                 output_transform=lambda x: x,\n",
    "                 check_compute_fn: bool = False):\n",
    "#         print(output_transform)\n",
    "        super(ROC_AUC_perClass,\n",
    "              self).__init__(roc_auc_perClass_compute_fn,\n",
    "                             output_transform=output_transform,\n",
    "                             check_compute_fn=check_compute_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def conv_output_shape(h_w, kernel_size=1, stride=1, pad=0, dilation=1):\n",
    "    \"\"\"\n",
    "  Utility function for computing output of convolutions\n",
    "  takes a tuple of (h,w) and returns a tuple of (h,w)\n",
    "  \"\"\"\n",
    "    from math import floor\n",
    "    if type(kernel_size) is not tuple:\n",
    "        kernel_size = (kernel_size, kernel_size)\n",
    "    h = floor(((h_w[0] + (2 * pad) - (dilation *\n",
    "                                      (kernel_size[0] - 1)) - 1) / stride) + 1)\n",
    "    w = floor(((h_w[1] + (2 * pad) - (dilation *\n",
    "                                      (kernel_size[1] - 1)) - 1) / stride) + 1)\n",
    "    return h, w\n",
    "\n",
    "# mel.shape,an_x.shape,X_train.shape\n",
    "class testModel(nn.Module):\n",
    "    '''A simple model for testing by overfitting.\n",
    "    '''\n",
    "    def __init__(self, out_channels, h_w, kernel_size, FLAT=False,output_shape=(10,)):\n",
    "        # h_w: height will be always one since we use 1d convolution \n",
    "        super(testModel, self).__init__()\n",
    "        self.out_channels = out_channels\n",
    "        #### CONV\n",
    "        self.conv1 = nn.Conv1d(in_channels=1, # depth of image == depth of filters\n",
    "                               out_channels=self.out_channels, # number of filters \n",
    "                               kernel_size=kernel_size, # size of the filters/kernels\n",
    "                               padding=1)\n",
    "\n",
    "        self.conv1_shape = conv_output_shape(h_w, kernel_size=kernel_size, stride=1, pad=1, dilation=1)\n",
    "        # conv is 1d\n",
    "        self.conv1_shape = (1,self.conv1_shape[1])\n",
    "        \n",
    "        self.fc1 = nn.Linear(self.out_channels * self.conv1_shape[0] *self.conv1_shape[1], 75)  # 100\n",
    "\n",
    "        self.fc2 = nn.Linear(75,output_shape[0])\n",
    "        \n",
    "        \n",
    "        torch.nn.init.xavier_uniform_(self.conv1.weight)\n",
    "        torch.nn.init.xavier_uniform_(self.fc1.weight)\n",
    "        torch.nn.init.xavier_uniform_(self.fc2.weight)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "#         x = x.reshape(1,)\n",
    "#         print(x.shape) #  50,1,108800 (850*128)\n",
    "        x = F.relu(self.conv1(x))\n",
    "#         x = self.pool(x)\n",
    "        # x = self.drop(x)\n",
    "#         print(x.shape)# 58, 2, 108801\n",
    "#         print(self.conv1_shape)\n",
    "#         print(x.shape)\n",
    "        x = x.view(-1, self.out_channels * self.conv1_shape[0] *self.conv1_shape[1])\n",
    "        # batch_norm is missing\n",
    "        x = F.relu((self.fc1(x)))\n",
    "        x = (self.fc2(x))\n",
    "\n",
    "#         x = self.drop(x)\n",
    "\n",
    "#         x = self.fc4(x)\n",
    "#         x = torch.sigmoid(x)\n",
    "#                 x = F.log_softmax(x,dim=1)\n",
    "        return x\n",
    "\n",
    "# # test\n",
    "# input_shape=(1,(938*128))\n",
    "# output_shape=(10,)\n",
    "# testModel_ins=testModel(out_channels=2,h_w=input_shape,kernel_size=2,output_shape=output_shape)\n",
    "# # a.conv1.weight\n",
    "# a_out=testModel_ins(torch.ones((3,1,input_shape[1])))\n",
    "\n",
    "# a_out_correct=torch.zeros(a_out.shape)\n",
    "# a_out_correct[0][:]=1\n",
    "# a_out_correct\n",
    "# a_out.detach().numpy()\n",
    "\n",
    "# torch.exp(a_out),a_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# for y_true_perClass_Index in a_out_correct.shape[1]:\n",
    "#     print(y_true_perClass_Index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from ignite.contrib.metrics import ROC_AUC\n",
    "# from nna.exp.metrics import ROC_AUC_perClass\n",
    "def activated_output_transform(output):\n",
    "    y_pred, y = output\n",
    "#     y_pred = torch.exp(y_pred)\n",
    "    return y_pred, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# asd=ROC_AUC_perClass(activated_output_transform)\n",
    "# asd.update((a_out,a_out_correct))\n",
    "# asd.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "transformCompose = transforms.Compose([\n",
    "#     pitch,\n",
    "#     strech,\n",
    "#     shift,\n",
    "#     noise,\n",
    "    toTensor,\n",
    "])\n",
    "\n",
    "\n",
    "sound_datasets = {\n",
    "    phase: runutils.audioDataset(XY[0], XY[1], transform=transformCompose)\n",
    "    for phase, XY in\n",
    "    zip(['train', 'val', 'test'],\n",
    "        [[X_train, y_train], [X_val, y_val], [X_test, y_test]])\n",
    "}\n",
    "\n",
    "dataloaders = {\n",
    "    x: torch.utils.data.DataLoader(sound_datasets[x], **params)\n",
    "    for x in ['train', 'val', 'test']\n",
    "}\n",
    "\n",
    "# this will change\n",
    "h_w = [128, 938]\n",
    "kernel_size = (5, 5)\n",
    "# if config['CNNLayer_count'] == 1:\n",
    "#     model = modelArchs.NetCNN1(config['CNN_filters_1'], h_w,\n",
    "#                                 kernel_size).float().to(device)\n",
    "\n",
    "# if config['CNNLayer_count'] == 2:\n",
    "#     model = modelArchs.NetCNN2(config['CNN_filters_1'], config.CNN_filters_2,\n",
    "#                                 h_w, kernel_size,\n",
    "#                                 kernel_size).float().to(device)\n",
    "\n",
    "#simpler model\n",
    "\n",
    "output_shape=(CATEGORY_COUNT,)\n",
    "model = testModel(out_channels=2,h_w=(1,h_w[0]*h_w[1]),kernel_size=kernel_size[0]*kernel_size[0],output_shape=output_shape)\n",
    "model.float().to(device)\n",
    "\n",
    "# device is defined before\n",
    "\n",
    "model.float().to(device)  # Move model before creating optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(),\n",
    "#                                 weight_decay=config['weight_decay'],\n",
    "                              amsgrad=True,\n",
    "                             )\n",
    "\n",
    "\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "# statHistory={'valLoss':[],'trainLoss':[],'trainAUC':[],'valAUC':[]}\n",
    "\n",
    "metrics = {\n",
    "    'loss': Loss(criterion),  # 'accuracy': Accuracy(),\n",
    "#     'ROC_AUC': ROC_AUC(runutils.activated_output_transform),\n",
    "    'ROC_AUC': ROC_AUC_perClass(activated_output_transform),\n",
    "\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "testModel(\n",
       "  (conv1): Conv1d(1, 2, kernel_size=(25,), stride=(1,), padding=(1,))\n",
       "  (fc1): Linear(in_features=240084, out_features=75, bias=True)\n",
       "  (fc2): Linear(in_features=75, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[-0.1655, -0.0348,  0.0037, -0.0861,  0.2388, -0.2812, -0.0201,\n",
       "           0.1076, -0.1862, -0.2773,  0.0596,  0.0522, -0.1000, -0.2006,\n",
       "          -0.1242, -0.0324,  0.0544, -0.0058,  0.0053,  0.0320,  0.2231,\n",
       "           0.0766, -0.2167,  0.2076,  0.1998]],\n",
       "\n",
       "        [[ 0.0346,  0.0648, -0.1567,  0.2104, -0.1485,  0.1780, -0.0413,\n",
       "           0.0800,  0.1863,  0.2529,  0.1293, -0.0186, -0.0285, -0.0969,\n",
       "           0.0148, -0.2024, -0.1476, -0.2799, -0.2271,  0.0679,  0.2659,\n",
       "           0.1187, -0.1989,  0.1877,  0.0064]]], device='cuda:1',\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.conv1.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv1d(1, 2, kernel_size=(25,), stride=(1,), padding=(1,))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.conv1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0007, -0.0033, -0.0009,  ..., -0.0034, -0.0042,  0.0023],\n",
       "        [-0.0037,  0.0028,  0.0015,  ..., -0.0014, -0.0025, -0.0016],\n",
       "        [-0.0012,  0.0021,  0.0001,  ...,  0.0049,  0.0011, -0.0025],\n",
       "        ...,\n",
       "        [ 0.0035, -0.0036, -0.0008,  ..., -0.0031,  0.0011,  0.0029],\n",
       "        [-0.0037,  0.0005,  0.0012,  ..., -0.0047,  0.0044, -0.0038],\n",
       "        [ 0.0009,  0.0040,  0.0013,  ...,  0.0014, -0.0030,  0.0012]],\n",
       "       device='cuda:1', requires_grad=True)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fc1.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0407, -0.0883, -0.0618,  0.1564,  0.1131, -0.0413, -0.1120, -0.1032,\n",
       "          0.2342, -0.2008, -0.0285,  0.1741, -0.1752,  0.0395,  0.2085, -0.0790,\n",
       "          0.1583,  0.2439, -0.0177,  0.2277,  0.1519, -0.0567, -0.1576,  0.1256,\n",
       "          0.1472, -0.2423,  0.0648, -0.0325,  0.1111,  0.0138,  0.2674,  0.1202,\n",
       "         -0.1830, -0.0928, -0.1099, -0.1313,  0.1599, -0.1447,  0.1281, -0.2193,\n",
       "          0.0106, -0.2092, -0.1733, -0.0967, -0.0441,  0.0586, -0.0745, -0.2231,\n",
       "          0.0274, -0.2694, -0.1276, -0.0869, -0.0896,  0.2333, -0.0148, -0.0837,\n",
       "          0.0109,  0.2598,  0.1989, -0.2720, -0.0190, -0.1238, -0.1834, -0.2188,\n",
       "         -0.1620,  0.1346,  0.1273, -0.0995, -0.0975, -0.0159,  0.1240, -0.2533,\n",
       "          0.0304, -0.1887, -0.2329],\n",
       "        [ 0.2224, -0.0156, -0.1965,  0.0838,  0.0621, -0.1233,  0.1867,  0.2192,\n",
       "          0.1927, -0.0953,  0.0352,  0.0057, -0.0354, -0.2657, -0.1950, -0.0191,\n",
       "         -0.0105, -0.0878,  0.2389,  0.1291,  0.1345,  0.1464,  0.0463, -0.2410,\n",
       "         -0.1553,  0.2722, -0.2513, -0.2146, -0.1090, -0.2072,  0.1739, -0.0256,\n",
       "         -0.1907, -0.1017,  0.0751,  0.0400,  0.0635, -0.1201,  0.0607, -0.0196,\n",
       "         -0.0518,  0.0960, -0.2407,  0.0508, -0.1574,  0.1611, -0.2292,  0.0912,\n",
       "         -0.2528, -0.2286,  0.0468,  0.2775,  0.1522,  0.1719,  0.1319,  0.1590,\n",
       "          0.0493, -0.2257,  0.2738, -0.1708,  0.0390,  0.0251, -0.2468, -0.2202,\n",
       "         -0.0475, -0.1982,  0.1237,  0.1727,  0.1499, -0.1493,  0.1439,  0.0799,\n",
       "         -0.0018,  0.0498,  0.1389]], device='cuda:1', requires_grad=True)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fc2.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1313,  0.1599, -0.1447,  0.1281, -0.2193,  0.0106, -0.2092, -0.1733,\n",
       "         -0.0967, -0.0441,  0.0586, -0.0745, -0.2231,  0.0274, -0.2694, -0.1276,\n",
       "         -0.0869, -0.0896,  0.2333, -0.0148, -0.0837,  0.0109,  0.2598,  0.1989,\n",
       "         -0.2720],\n",
       "        [ 0.0400,  0.0635, -0.1201,  0.0607, -0.0196, -0.0518,  0.0960, -0.2407,\n",
       "          0.0508, -0.1574,  0.1611, -0.2292,  0.0912, -0.2528, -0.2286,  0.0468,\n",
       "          0.2775,  0.1522,  0.1719,  0.1319,  0.1590,  0.0493, -0.2257,  0.2738,\n",
       "         -0.1708]], device='cuda:1', grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fc2.weight[:,35:60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.2189263105392456\n",
      "-10.042251586914062\n",
      "-1.663476586341858\n",
      "-0.004378526005893946\n",
      "-5.577076649387891e-07\n",
      "-0.01108984462916851\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for m in (torch.sum(model.conv1.weight),torch.sum(model.fc1.weight),torch.sum(model.fc2.weight),\n",
    "torch.mean(model.conv1.weight),torch.mean(model.fc1.weight),torch.mean(model.fc2.weight)):\n",
    "    print(m.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#amsgrad=True, init.xavier_uniform_, 25,75\n",
    "\n",
    "print('ready ?')\n",
    "runutils.run(model, dataloaders, optimizer, criterion, metrics, device,config, runconfigs.PROJECT_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ignite.engine import create_supervised_evaluator\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_loader = dataloaders['test']\n",
    "# test_evaluator = create_supervised_evaluator(model,\n",
    "#                                               metrics=metrics,\n",
    "#                                               device=device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/enis/projects/nna/src/nna/exp/megan/run-1/wandb/run-20210114_155758-x6ltobzt/files/best_model_10_ROC_AUC=0.8853.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/enis/conda/envs/soundenv3/lib/python3.7/site-packages/torch/functional.py:516: UserWarning: stft will require the return_complex parameter be explicitly  specified in a future PyTorch release. Use return_complex=False  to preserve the current behavior or return_complex=True to return  a complex output. (Triggered internally at  /opt/conda/conda-bld/pytorch_1607370141920/work/aten/src/ATen/native/SpectralOps.cpp:653.)\n",
      "  normalized, onesided, return_complex)\n",
      "/scratch/enis/conda/envs/soundenv3/lib/python3.7/site-packages/torch/functional.py:516: UserWarning: The function torch.rfft is deprecated and will be removed in a future PyTorch release. Use the new torch.fft module functions, instead, by importing torch.fft and calling torch.fft.fft or torch.fft.rfft. (Triggered internally at  /opt/conda/conda-bld/pytorch_1607370141920/work/aten/src/ATen/native/SpectralOps.cpp:590.)\n",
      "  normalized, onesided, return_complex)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss 1.0067618106846261\n",
      "test roc auc [0.84182099 0.85300926]\n"
     ]
    }
   ],
   "source": [
    "model_files_path = '/home/enis/projects/nna/src/nna/exp/megan/run-1/wandb/run-20210114_155758-x6ltobzt/files/'\n",
    "model_path= model_files_path + 'best_model_9_ROC_AUC=0.8751.pt'\n",
    "model_list = glob.glob(model_files_path+'best_model*')\n",
    "model_list.sort()\n",
    "for model_path in model_list:\n",
    "    print(model_path)\n",
    "    model_saved = testModel(out_channels=2,h_w=(1,h_w[0]*h_w[1]),kernel_size=kernel_size[0]*kernel_size[0],output_shape=output_shape)\n",
    "    model_saved.load_state_dict(torch.load(model_path))\n",
    "    model_saved.eval().to(device)\n",
    "    test_loader = dataloaders['test']\n",
    "    test_evaluator = create_supervised_evaluator(model_saved,\n",
    "                                                  metrics=metrics,\n",
    "                                                  device=device)\n",
    "\n",
    "    test_evaluator.run(test_loader)\n",
    "    roc_auc_array_test = test_evaluator.state.metrics['ROC_AUC']\n",
    "\n",
    "    print('test loss', test_evaluator.state.metrics['loss'])\n",
    "    print('test roc auc', roc_auc_array_test, )\n",
    "    break\n",
    "\n",
    "# BEST\n",
    "# /home/enis/projects/nna/src/nna/exp/megan/run-1/wandb/run-20210114_155758-x6ltobzt/files/best_model_10_ROC_AUC=0.8853.pt\n",
    "# test loss 1.0067617787724048\n",
    "# test roc auc [0.84182099 0.85300926]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs=[]\n",
    "labelss=[]\n",
    "for inputs, labels in dataloaders['test']:\n",
    "    inputs = inputs.float().to(device)\n",
    "    output = model_saved(inputs)\n",
    "    output = output.to(\"cpu\")\n",
    "    index = output.data.numpy()\n",
    "    outputs.append(index)\n",
    "    labelss.append(labels)\n",
    "#     print(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41, 17)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t=0\n",
    "f=0\n",
    "limit=0\n",
    "for m,n in zip(outputs[0]>limit,labelss[0]>0):\n",
    "    if (m==n.numpy()).all():\n",
    "        t+=1\n",
    "    else:\n",
    "        f+=1\n",
    "t,f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ True, False],\n",
       "        [False,  True],\n",
       "        [ True, False],\n",
       "        [ True, False],\n",
       "        [ True, False],\n",
       "        [False,  True],\n",
       "        [ True, False],\n",
       "        [ True, False],\n",
       "        [ True, False],\n",
       "        [False,  True],\n",
       "        [ True, False],\n",
       "        [ True, False],\n",
       "        [ True, False],\n",
       "        [ True, False],\n",
       "        [ True, False],\n",
       "        [ True, False],\n",
       "        [ True, False],\n",
       "        [False,  True],\n",
       "        [ True, False],\n",
       "        [ True, False],\n",
       "        [ True, False],\n",
       "        [ True, False],\n",
       "        [ True, False],\n",
       "        [False,  True],\n",
       "        [ True, False],\n",
       "        [ True, False],\n",
       "        [False,  True],\n",
       "        [ True, False],\n",
       "        [ True, False],\n",
       "        [False,  True],\n",
       "        [False,  True],\n",
       "        [ True, False],\n",
       "        [False,  True],\n",
       "        [ True, False],\n",
       "        [ True, False],\n",
       "        [ True, False],\n",
       "        [False,  True],\n",
       "        [ True, False],\n",
       "        [ True, False],\n",
       "        [ True, False],\n",
       "        [ True, False],\n",
       "        [False,  True],\n",
       "        [ True, False],\n",
       "        [ True, False],\n",
       "        [ True, False],\n",
       "        [False,  True],\n",
       "        [ True, False],\n",
       "        [ True, False],\n",
       "        [ True, False],\n",
       "        [ True, False],\n",
       "        [False,  True],\n",
       "        [ True, False],\n",
       "        [False,  True],\n",
       "        [ True, False],\n",
       "        [ True, False],\n",
       "        [ True, False],\n",
       "        [ True, False],\n",
       "        [ True, False]])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labelss[0]>0,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.4041343 -3.5336268] tensor([1., 0.])\n",
      "[-4.721764   3.9302242] tensor([0., 1.])\n",
      "[-0.6255104  0.5269026] tensor([1., 0.])\n",
      "[ 3.0626864 -6.2202144] tensor([1., 0.])\n",
      "[-1.5258244  1.369283 ] tensor([1., 0.])\n",
      "[-1.1834712  1.7585473] tensor([0., 1.])\n",
      "[ 0.5676827 -0.9845088] tensor([1., 0.])\n",
      "[-0.23613587 -0.24668674] tensor([1., 0.])\n",
      "[-8.427179  8.767305] tensor([1., 0.])\n",
      "[ 31.213781 -31.929234] tensor([0., 1.])\n",
      "[ 2.3647826 -3.863914 ] tensor([1., 0.])\n",
      "[ 14.386643 -11.592027] tensor([1., 0.])\n",
      "[ 42.059864 -42.190323] tensor([1., 0.])\n",
      "[ 15.248289 -18.514023] tensor([1., 0.])\n",
      "[ 2.518043 -2.029641] tensor([1., 0.])\n",
      "[-0.2652089  0.6528234] tensor([1., 0.])\n",
      "[-3.255232   3.6832774] tensor([1., 0.])\n",
      "[-0.23310822  0.35905677] tensor([0., 1.])\n",
      "[ 4.0603647 -5.3932114] tensor([1., 0.])\n",
      "[ 2.0881953 -2.0108125] tensor([1., 0.])\n",
      "[-0.624428   -0.87871474] tensor([1., 0.])\n",
      "[ 7.186704  -7.6504793] tensor([1., 0.])\n",
      "[ 5.342203  -7.0520372] tensor([1., 0.])\n",
      "[-1.6651335  0.6895987] tensor([0., 1.])\n",
      "[-2.3494873   0.73684174] tensor([1., 0.])\n",
      "[ 11.817939 -11.47354 ] tensor([1., 0.])\n",
      "[0.38071376 0.9791849 ] tensor([0., 1.])\n",
      "[ 2.7657728 -1.9925255] tensor([1., 0.])\n",
      "[ 3.050109 -8.025728] tensor([1., 0.])\n",
      "[-6.5474687  4.799638 ] tensor([0., 1.])\n",
      "[-11.693979  10.396906] tensor([0., 1.])\n",
      "[-0.2818371   0.37543565] tensor([1., 0.])\n",
      "[-25.373913  21.972164] tensor([0., 1.])\n",
      "[ 2.5867674 -2.968048 ] tensor([1., 0.])\n",
      "[-0.6249446  1.1665802] tensor([1., 0.])\n",
      "[-1.1927465  1.6927372] tensor([1., 0.])\n",
      "[-4.455087   7.5972815] tensor([0., 1.])\n",
      "[-0.96945345  0.7871111 ] tensor([1., 0.])\n",
      "[ 1.6852297 -0.6973579] tensor([1., 0.])\n",
      "[ 4.0363784 -3.8032024] tensor([1., 0.])\n",
      "[ 0.17926675 -0.4186169 ] tensor([1., 0.])\n",
      "[-2.092931  1.992889] tensor([0., 1.])\n",
      "[ 0.9047496 -0.773653 ] tensor([1., 0.])\n",
      "[ 7.831672 -7.310137] tensor([1., 0.])\n",
      "[-4.8378263  4.5872884] tensor([1., 0.])\n",
      "[-20.21518   18.759106] tensor([0., 1.])\n",
      "[ 9.63065  -9.281886] tensor([1., 0.])\n",
      "[ 2.655655  -1.1492065] tensor([1., 0.])\n",
      "[ 3.4041545 -3.554578 ] tensor([1., 0.])\n",
      "[ 2.0189123 -1.5936447] tensor([1., 0.])\n",
      "[-10.719267   9.259364] tensor([0., 1.])\n",
      "[ 4.170948  -2.1986516] tensor([1., 0.])\n",
      "[-2.670876   6.9447794] tensor([0., 1.])\n",
      "[-1.7331527  1.4263479] tensor([1., 0.])\n",
      "[ 7.386463 -5.735483] tensor([1., 0.])\n",
      "[ 2.3208442 -3.4700253] tensor([1., 0.])\n",
      "[ 2.5700297 -3.3384216] tensor([1., 0.])\n",
      "[-2.155287   2.1744254] tensor([1., 0.])\n"
     ]
    }
   ],
   "source": [
    "for m,n in zip(outputs[0],labelss[0]):\n",
    "    print(m,n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss 0.575299070472211\n",
      "test roc auc [0.73958333 0.73533951]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[-0.1916, -0.1012, -0.0548, -0.1832,  0.0159, -0.1202,  0.2493,\n",
       "           0.1585,  0.2155, -0.2738, -0.0770,  0.1383, -0.2549,  0.0978,\n",
       "           0.2230,  0.0194, -0.0346,  0.0620, -0.2393, -0.0458, -0.1269,\n",
       "          -0.1478, -0.1659, -0.0260, -0.0971]],\n",
       "\n",
       "        [[-0.1530,  0.1615, -0.0582, -0.2440, -0.1321,  0.1978, -0.0892,\n",
       "          -0.2765,  0.0416, -0.2716,  0.1296, -0.1338,  0.0496,  0.2502,\n",
       "          -0.1181,  0.2220,  0.2381, -0.1465, -0.0384, -0.1267,  0.0831,\n",
       "          -0.1753, -0.2311, -0.2757,  0.2426]]], device='cuda:1',\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.conv1.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.28379660844802856\n",
      "-35989.8203125\n",
      "-1.550400972366333\n",
      "-0.005675931926816702\n",
      "-0.0019987348932772875\n",
      "-0.010336006991565228\n"
     ]
    }
   ],
   "source": [
    "for m in (torch.sum(model.conv1.weight),torch.sum(model.fc1.weight),torch.sum(model.fc2.weight),\n",
    "torch.mean(model.conv1.weight),torch.mean(model.fc1.weight),torch.mean(model.fc2.weight)):\n",
    "    print(m.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.8156369924545288\n",
      "-2.30564022064209\n",
      "1.113730788230896\n",
      "-0.03631274029612541\n",
      "-1.2804630955542962e-07\n",
      "0.007424871902912855\n"
     ]
    }
   ],
   "source": [
    "for m in (torch.sum(model.conv1.weight),torch.sum(model.fc1.weight),torch.sum(model.fc2.weight),\n",
    "torch.mean(model.conv1.weight),torch.mean(model.fc1.weight),torch.mean(model.fc2.weight)):\n",
    "    print(m.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[-0.2487, -0.1000, -0.2389, -0.0892,  0.0787,  0.2289,  0.0016,\n",
       "           0.1942,  0.2148,  0.0233, -0.2194, -0.1379,  0.2334, -0.2496,\n",
       "           0.1046,  0.0043,  0.1701, -0.0154,  0.2012,  0.0757, -0.2177,\n",
       "           0.1922, -0.1940,  0.0119, -0.2180]],\n",
       "\n",
       "        [[-0.1250, -0.1686, -0.1103, -0.0830, -0.0704,  0.2523, -0.0615,\n",
       "          -0.0895, -0.0651, -0.2415, -0.0692, -0.0974, -0.2820,  0.0661,\n",
       "           0.1650,  0.1759,  0.2147, -0.2037,  0.1802,  0.1424, -0.0963,\n",
       "           0.2779,  0.1434, -0.2229,  0.2743]]], device='cuda:1',\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.conv1.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-6.3913e-03, -3.9652e-04,  1.5468e-03,  ..., -3.7154e-03,\n",
       "         -4.6094e-03,  4.2543e-03],\n",
       "        [-5.4962e-03, -3.2947e-03, -9.8121e-03,  ...,  4.6668e-03,\n",
       "         -1.4585e-03,  1.4293e-03],\n",
       "        [-1.3380e-03, -8.9816e-03, -1.0979e-02,  ...,  1.5270e-03,\n",
       "         -4.9358e-03,  1.2266e-03],\n",
       "        ...,\n",
       "        [-5.4898e-03, -9.9627e-03, -5.8345e-03,  ...,  3.9291e-03,\n",
       "         -3.1578e-03, -2.2261e-03],\n",
       "        [ 3.1439e-02, -3.5185e-03, -9.0754e-03,  ...,  6.5032e-05,\n",
       "         -3.0601e-03,  3.2819e-04],\n",
       "        [-6.9045e-03,  1.5367e-03, -7.8508e-03,  ..., -3.6295e-03,\n",
       "         -1.8707e-03,  2.7540e-03]], device='cuda:1', requires_grad=True)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fc1.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.0258,  0.1852,  0.2271,  0.1194,  0.1158,  0.0520, -0.2124,  0.2611,\n",
       "          0.1001,  0.2018, -0.0867, -0.0450, -0.2670, -0.0627,  0.1312,  0.1560,\n",
       "         -0.1971,  0.2356, -0.2341,  0.2606,  0.0816, -0.1185,  0.1474,  0.2373,\n",
       "         -0.1709, -0.1620,  0.2559, -0.1174, -0.2700, -0.3141,  0.1292,  0.1363,\n",
       "          0.2860,  0.1132, -0.0830, -0.0842,  0.0821,  0.1297,  0.1850,  0.1338,\n",
       "          0.1675, -0.2371, -0.1067, -0.1755,  0.1127,  0.1066, -0.0880,  0.2650,\n",
       "         -0.2147, -0.0141,  0.1902, -0.0046,  0.0631,  0.0874, -0.0506, -0.2295,\n",
       "         -0.1659,  0.0087,  0.2414,  0.0652,  0.1240,  0.2610,  0.2491,  0.0604,\n",
       "         -0.2192,  0.0183,  0.1401,  0.0713, -0.1225,  0.0926, -0.0011, -0.1932,\n",
       "         -0.0828,  0.0607,  0.2029],\n",
       "        [ 0.1402, -0.0403, -0.1288,  0.1159, -0.2280, -0.0907,  0.1757, -0.0119,\n",
       "          0.1990, -0.0768,  0.0439, -0.0339,  0.1010,  0.1045, -0.2024,  0.2035,\n",
       "         -0.2426,  0.0380,  0.2121,  0.2308,  0.1837, -0.2347, -0.1553,  0.1988,\n",
       "         -0.0712,  0.2237, -0.2365, -0.1505,  0.2253,  0.1813, -0.1458, -0.2366,\n",
       "         -0.2428,  0.1527,  0.1102,  0.2844, -0.1566,  0.1350,  0.1791,  0.0494,\n",
       "         -0.0752,  0.2021, -0.1254, -0.1236, -0.0041,  0.0530, -0.0815,  0.2095,\n",
       "          0.2123, -0.0700,  0.2474, -0.0100,  0.0983, -0.2826, -0.0408,  0.0087,\n",
       "         -0.0732, -0.2170,  0.0904,  0.1072,  0.1762,  0.2714, -0.1775, -0.0512,\n",
       "          0.2502, -0.2266,  0.1925, -0.1293,  0.0537, -0.1019,  0.2081,  0.1620,\n",
       "         -0.0510, -0.0726,  0.1690]], device='cuda:1', requires_grad=True)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fc2.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "testModel(\n",
       "  (conv1): Conv1d(1, 2, kernel_size=(25,), stride=(1,), padding=(1,))\n",
       "  (fc1): Linear(in_features=240084, out_features=75, bias=True)\n",
       "  (fc2): Linear(in_features=75, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "example_input = (torch.ones((1,1,h_w[0]*h_w[1]))*-32767)\n",
    "example_input = example_input.float().to(device)\n",
    "out=model(example_input)\n",
    "out\n",
    "\n",
    "activated_output_transform((out,out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.Tensor([10,10,10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(model.fc1.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = nn.Sigmoid()\n",
    "\n",
    "loss = nn.BCEWithLogitsLoss()\n",
    "input = torch.tensor([100.0,100,100], requires_grad=True)\n",
    "# target = torch.empty(3).random_(2)\n",
    "target = torch.ones(3)\n",
    "output = loss((input), target)\n",
    "input,target,output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "soundenv3",
   "language": "python",
   "name": "soundenv3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

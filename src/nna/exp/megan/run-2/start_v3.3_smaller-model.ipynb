{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make model smaller, \n",
    "\n",
    "import os\n",
    "\n",
    "os.chdir('/home/enis/projects/nna/src/nna/exp/megan/run-2/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/enis/conda/envs/soundenv3/lib/python3.7/site-packages/torchaudio/backend/utils.py:54: UserWarning: \"sox\" backend is being deprecated. The default backend will be changed to \"sox_io\" backend in 0.8.0 and \"sox\" backend will be removed in 0.9.0. Please migrate to \"sox_io\" backend. Please refer to https://github.com/pytorch/audio/issues/903 for the detail.\n",
      "  '\"sox\" backend is being deprecated. '\n"
     ]
    }
   ],
   "source": [
    "# import run\n",
    "# import nna\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "import torchaudio\n",
    "torchaudio.set_audio_backend(\"sox_io\")\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from pathlib import Path\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_per_set = [['45',\n",
    "  '38',\n",
    "  '48',\n",
    "  '39',\n",
    "  '11',\n",
    "  '44',\n",
    "  '46',\n",
    "  '17',\n",
    "  '20',\n",
    "  '50',\n",
    "  '13',\n",
    "  '25',\n",
    "  '21',\n",
    "  '29',\n",
    "  '19',\n",
    "  '16',\n",
    "  '24',\n",
    "  '37'],\n",
    " ['18', '31', '34', '27', '32', '33', '47', '41', '22', '15'],\n",
    " ['30', '12', '14', '36', '40', '49']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import runconfigs\n",
    "import wandb\n",
    "from ignite.metrics import Accuracy, Loss\n",
    "from ignite.contrib.metrics import ROC_AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nna.exp import augmentations,\n",
    "from nna.exp import modelArchs,runutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandb.init(config=runconfigs.default_config, project=runconfigs.PROJECT_NAME)\n",
    "# config = wandb.config\n",
    "config = runconfigs.default_config\n",
    "# wandb.config.update(args) # adds all of the arguments as config variables\n",
    "config['batch_size'] = 64\n",
    "params = {\n",
    "    'batch_size': config['batch_size'],\n",
    "    'shuffle': True,\n",
    "    'num_workers': 0\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\n",
    "    f\"cuda:{config['device']}\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# labelsbyhumanpath = Path('/scratch/enis/data/nna/labeling/results/')\n",
    "# sourcePath = Path(\"/scratch/enis/data/nna/labeling/splits/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "CATEGORY_COUNT = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "job_logs\t\t\t   start_v3.2.2_smaller-model.ipynb\n",
      "__pycache__\t\t\t   start_v3.2_smaller-model.ipynb\n",
      "runconfigs.py\t\t\t   start_v3.3_smaller-model.ipynb\n",
      "run.py\t\t\t\t   start_v3_split-loc.ipynb\n",
      "start_v3.1_negative-samples.ipynb  wandb\n",
      "start_v3.2.1_smaller-model.ipynb\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/scratch/enis/data/nna/labeling/megan/AudioSamplesPerSite/meganLabeledFiles_wlenV1.txt\n",
      "/scratch/enis/data/nna/labeling/megan/AudioSamplesPerSite\n",
      "4 files are missing corresponding to excell entries\n",
      "'-> 5 number of samples are DELETED due to ignore_files and missing_audio_files'\n",
      "-> 415 samples DELETED because they are not in the excell\n",
      "\n",
      "-> 0 samples DELETED because they do not have the taxo info coming from excell\n",
      "\n",
      "-> classes that do not have enough data:\n",
      "[REMOVED!]\n",
      "['other-mammal'] 0.0\n",
      "['other-silence'] 20.0\n",
      "['unknown-sound'] 2.0\n",
      "['seabirds'] 1.0\n",
      "['canids'] 1.0\n",
      "['other-flare'] 11.0\n",
      "['other-rain'] 20.0\n",
      "\n",
      "-> classes that have enough data:\n",
      "['other-biophony'] 56.0\n",
      "['other-insect'] 140.0\n",
      "['other-bird'] 661.0\n",
      "['songbirds'] 392.0\n",
      "['duck-goose-swan'] 183.0\n",
      "['grouse-ptarmigan'] 59.0\n",
      "['other-anthrophony'] 66.0\n",
      "['other-aircraft'] 107.0\n",
      "['loons'] 29.0\n",
      "['other-car'] 37.0\n",
      "('-> 102 number of samples are deleted because their taxonomy category does '\n",
      " 'not have enough data.')\n",
      "-> classes that do not have enough data\n",
      "will be REMOVED!\n",
      "-> 97 number of samples are deleted because their length is not long enough.\n",
      "loading from cache at /scratch/enis/data/nna/labeling/megan/AudioSamplesPerSite/files_as_np_filtered_v3_int16.pkl\n"
     ]
    }
   ],
   "source": [
    "## Load real data rather than mock \n",
    "    # MVP1: delete parts longer than 10 seconds\n",
    "import run\n",
    "audio_dataset,_ = run.prepare_dataset()\n",
    "\n",
    "output_file_path = '/scratch/enis/data/nna/labeling/megan/AudioSamplesPerSite/files_as_np_filtered_v3_int16.pkl'\n",
    "audio_dataset.load_audio_files(output_file_path)\n",
    "audio_dataset.pick_channel_by_clipping()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sound_ins=next(iter(audio_dataset.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sound_ins[1].location_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sound_ins[1].taxo_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(832, 832)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sound_ins[1].taxo_code\n",
    "# classA = 1.1.7 #'duck-goose-swan']\n",
    "# classB = 0.2.0 # other-aircraft\n",
    "# 3.0.0 : 0.48, 0.26, 0.26, 46 # silence\n",
    "# 2.1.0 : 0.22, 0.56, 0.22, 18 # rain\n",
    "# 1.3.0 1.3.0 : 0.52, 0.4, 0.087, 161 # insect\n",
    "# 1.1.8 : 0.49, 0.19, 0.32, 88 # grouse-ptarmigan\n",
    "\n",
    "other_taxo = ['3.0.0','2.1.0','1.3.0','1.1.8']\n",
    "\n",
    "sampleTest= []\n",
    "y=[]\n",
    "location_id_info = []\n",
    "expected_len=10\n",
    "for sound_ins in audio_dataset.values():\n",
    "    if sound_ins.taxo_code in ['1.1.10','1.1.7'] + other_taxo:\n",
    "        y.append(sound_ins.taxo_code)\n",
    "        location_id_info.append(sound_ins.location_id)\n",
    "        if sound_ins.length<10:\n",
    "            tile_reps = (expected_len/(sound_ins.length)+1)\n",
    "            repeated_data = np.tile(sound_ins.data,int(tile_reps))\n",
    "            repeated_data = repeated_data[:expected_len*sound_ins.sr]\n",
    "            sampleTest.append(repeated_data)\n",
    "        else:\n",
    "            sampleTest.append(sound_ins.data[:expected_len*sound_ins.sr])\n",
    "\n",
    "len(sampleTest),len(y)\n",
    "\n",
    "# sampleTest=np.array(sampleTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(902, 0.5314091680814941)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "313+589,(46+18+161+88)/589 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import argmax\n",
    "# define input string\n",
    "# define universe of possible input values\n",
    "alphabet = ['1.1.10','1.1.7']\n",
    "# define a mapping of chars to integers\n",
    "char_to_int = dict((c, i) for i, c in enumerate(alphabet))\n",
    "int_to_char = dict((i, c) for i, c in enumerate(alphabet))\n",
    "# integer encode input data\n",
    "integer_encoded = [char_to_int.get(char,None) for char in y]\n",
    "# print(integer_encoded)\n",
    "# one hot encode\n",
    "onehot_encoded = list()\n",
    "for value in integer_encoded:\n",
    "\tletter = [0 for _ in range(len(alphabet))]\n",
    "\tif value is not None:\n",
    "\t\tletter[value] = 1\n",
    "\tonehot_encoded.append(letter)\n",
    "# print(onehot_encoded)\n",
    "# invert encoding\n",
    "inverted = int_to_char[argmax(onehot_encoded[0])]\n",
    "# print(inverted)\n",
    "onehot_encoded=np.array(onehot_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test, X_val, y_train, y_test,y_val  = [],[],[],[],[],[]\n",
    "loc_id_train=[]\n",
    "loc_id_test=[]\n",
    "loc_id_valid=[]\n",
    "\n",
    "for sample,y_val_ins,loc_id in  zip(sampleTest,onehot_encoded,location_id_info):\n",
    "    if loc_id in loc_per_set[0]:\n",
    "        X_train.append(sample)\n",
    "        y_train.append(y_val_ins)\n",
    "        loc_id_train.append(loc_id)\n",
    "    elif loc_id in loc_per_set[1]:\n",
    "        X_test.append(sample)\n",
    "        y_test.append(y_val_ins)\n",
    "        loc_id_test.append(loc_id)\n",
    "    elif loc_id in loc_per_set[2]:\n",
    "        X_val.append(sample)\n",
    "        y_val.append(y_val_ins)\n",
    "        loc_id_valid.append(loc_id)\n",
    "    else:\n",
    "        print('error')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'11',\n",
       "  '16',\n",
       "  '17',\n",
       "  '19',\n",
       "  '20',\n",
       "  '21',\n",
       "  '24',\n",
       "  '25',\n",
       "  '29',\n",
       "  '37',\n",
       "  '38',\n",
       "  '39',\n",
       "  '44',\n",
       "  '46',\n",
       "  '48',\n",
       "  '50'},\n",
       " {'15', '18', '22', '27', '31', '32', '33', '41', '47'},\n",
       " {'14', '30', '40', '49'})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(loc_id_train),set(loc_id_test),set(loc_id_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "832"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sampleTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,X_val=np.array(X_train),np.array(X_test),np.array(X_val)\n",
    "y_train,y_test,y_val=np.array(y_train),np.array(y_test),np.array(y_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_test, y_train, y_test = train_test_split(\n",
    "#                 sampleTest, onehot_encoded, test_size=0.2, random_state=42)\n",
    "# X_train, X_val, y_train, y_val = train_test_split(\n",
    "#                 X_train, y_train, test_size=0.25,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,X_val=torch.from_numpy(X_train).float(),torch.from_numpy(X_test).float(),torch.from_numpy(X_val).float()\n",
    "y_train,y_test,y_val=torch.from_numpy(y_train).float(),torch.from_numpy(y_test).float(),torch.from_numpy(y_val).float()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([459, 480000]),\n",
       " torch.Size([216, 480000]),\n",
       " torch.Size([157, 480000]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape,X_test.shape,X_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([459, 2]), torch.Size([216, 2]), torch.Size([157, 2]))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape,y_test.shape,y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_val[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # X_train,X_test,X_val,y_train,y_test,y_val = load_raw_data('labelsbyhumanpath','sourcePath')\n",
    "# X_train,X_test,X_val,y_train,y_test,y_val = mock_raw_data(4,CATEGORY_COUNT)\n",
    "# X_train,X_test,X_val=torch.from_numpy(X_train).float(),torch.from_numpy(X_test).float(),torch.from_numpy(X_val).float()\n",
    "# y_train,y_test,y_val=torch.from_numpy(y_train).float(),torch.from_numpy(y_test).float(),torch.from_numpy(y_val).float()\n",
    "\n",
    "# # labelsbyhumanpath = Path('/scratch/enis/data/nna/labeling/results/')\n",
    "# # with open(labelsbyhumanpath/\"np_array_Ymatrix.npy\", 'rb') as f:\n",
    "# #     y_true = np.load(f)\n",
    "\n",
    "# Counter(np.argmax(y_train,axis=1).tolist()),Counter(np.argmax(y_val,axis=1).tolist()),Counter(np.argmax(y_test,axis=1).tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# X_train2=np.interp(X_train, (X_train.min(), X_train.max()), (-32768 , 32767))\n",
    "# torch.from_numpy(X_train2).float()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# smaple_index=10\n",
    "# X_train[smaple_index,:],y_train[smaple_index,]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train[0,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multiply(torch.ones((1,2)),torch.ones((1,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToTensor(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "\n",
    "    def __init__(self, maxMelLen, sampling_rate):\n",
    "        # sr = 44100 etc\n",
    "        self.maxMelLen = maxMelLen\n",
    "        self.sampling_rate = sampling_rate\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        x, y = sample\n",
    "        #https://github.com/PCerles/audio/blob/3803d0b27a4e13efa760227ef6c71d0f3753aa98/test/test_transforms.py#L262\n",
    "        #librosa defaults\n",
    "        n_fft = 2048\n",
    "        hop_length = 512\n",
    "        power = 2.0\n",
    "        n_mels = 128\n",
    "        n_mfcc = 40\n",
    "        # htk is false in librosa, no setting in torchaudio -?\n",
    "        # norm is 1 in librosa, no setting in torchaudio -?\n",
    "        melspect_transform = torchaudio.transforms.MelSpectrogram(sample_rate=self.sampling_rate, window_fn=torch.hann_window,\n",
    "                                                                  hop_length=hop_length, n_mels=n_mels, n_fft=n_fft)\n",
    "\n",
    "    \n",
    "        db_transform = torchaudio.transforms.AmplitudeToDB(\"power\", 80.)\n",
    "        mel = melspect_transform(x.reshape(-1))\n",
    "        an_x = db_transform(mel)\n",
    "        #librosa version\n",
    "#         mel = librosa.feature.melspectrogram(y=x.reshape(-1),\n",
    "#                                              sr=self.sampling_rate)\n",
    "#         an_x = librosa.power_to_db(mel, ref=np.max)\n",
    "#         an_x = an_x.astype(\"float32\")\n",
    "#         y = y.astype('float32')\n",
    "#         print(an_x.shape)\n",
    "        an_x = an_x[:, :self.maxMelLen]\n",
    "        # 2-d conv\n",
    "#         x = an_x.reshape(1, *an_x.shape[:])\n",
    "        # 1-d conv\n",
    "        x = an_x.reshape(1, an_x.shape[0]*an_x.shape[1])\n",
    "\n",
    "        \n",
    "        return x,y\n",
    "\n",
    "# #test\n",
    "# maxMelLen_test = 850\n",
    "# SAMPLING_RATE_test = 48000\n",
    "# sample_len_seconds = 10\n",
    "# # to_tensor works on single sample\n",
    "# sample_count = 1\n",
    "# xx_test = torch.ones((sample_count,SAMPLING_RATE_test*sample_len_seconds))\n",
    "# y_values = torch.ones(sample_count)\n",
    "# \n",
    "# toTensor = ToTensor(maxMelLen_test,SAMPLING_RATE_test)\n",
    "# x_out,y_out=toTensor((xx_test,y_values))\n",
    "# x_out.shape,y_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([459, 480000])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X_train,X_test,X_val,y_train,y_test,y_val\n",
    "X_train[:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# toTensor = ToTensor(maxMelLen_test,SAMPLING_RATE_test)\n",
    "# x_out2,y_out=toTensor((X_train[1:2,:],y_train))\n",
    "# x_out.shape,y_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.mean(x_out[0]),torch.mean(x_out2[0])\n",
    "# y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train,y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# pitch = augmentations.pitch_shift_n_stepsClass(\n",
    "#     runconfigs.SAMPLING_RATE, config['pitch_shift_n_steps'])\n",
    "# noise = augmentations.addNoiseClass(config['noise_factor'])\n",
    "# strech = augmentations.time_stretchClass(runconfigs.SAMPLING_RATE*runconfigs.EXCERPT_LENGTH,\n",
    "#                                             config['time_stretch_factor'],\n",
    "#                                             isRandom=True)\n",
    "# shift = augmentations.shiftClass(config['roll_rate'], isRandom=True)\n",
    "maxMelLen = 938 # old 850\n",
    "# toTensor = augmentations.ToTensor(maxMelLen,runconfigs.SAMPLING_RATE)\n",
    "toTensor = ToTensor(maxMelLen,runconfigs.SAMPLING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ignite.metrics import EpochMetric\n",
    "\n",
    "\n",
    "def roc_auc_perClass_compute_fn(y_preds, y_targets):\n",
    "    try:\n",
    "        from sklearn.metrics import roc_auc_score\n",
    "    except ImportError:\n",
    "        raise RuntimeError(\n",
    "            \"This contrib module requires sklearn to be installed.\")\n",
    "\n",
    "    y_true = y_targets.numpy()\n",
    "    y_pred = y_preds.numpy()\n",
    "#     print(y_pred,y_true)\n",
    "#     res = []\n",
    "#     for y_true_perClass_Index in y_true.shape[1]:\n",
    "#         res.append(\n",
    "#             roc_auc_score(y_true[:, y_true_perClass_Index],\n",
    "#                           y_pred[:, y_true_perClass_Index]))\n",
    "    res = roc_auc_score(y_true, y_pred, average=None)\n",
    "    return res\n",
    "\n",
    "\n",
    "#[docs]\n",
    "class ROC_AUC_perClass(EpochMetric):\n",
    "    \"\"\"Computes Area Under the Receiver Operating Characteristic Curve (ROC AUC)\n",
    "  accumulating predictions and the ground-truth during an epoch and applying\n",
    "  `sklearn.metrics.roc_auc_score <http://scikit-learn.org/stable/modules/generated/\n",
    "  sklearn.metrics.roc_auc_score.html#sklearn.metrics.roc_auc_score>`_ .\n",
    "\n",
    "  Args:\n",
    "      output_transform (callable, optional): a callable that is used to transform the\n",
    "          :class:`~ignite.engine.engine.Engine`'s ``process_function``'s output into the\n",
    "          form expected by the metric. This can be useful if, for example, you have a multi-output model and\n",
    "          you want to compute the metric with respect to one of the outputs.\n",
    "      check_compute_fn (bool): Optional default False. If True, `roc_curve\n",
    "          <http://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html#\n",
    "          sklearn.metrics.roc_auc_score>`_ is run on the first batch of data to ensure there are\n",
    "          no issues. User will be warned in case there are any issues computing the function.\n",
    "\n",
    "  ROC_AUC expects y to be comprised of 0's and 1's. y_pred must either be probability estimates or confidence\n",
    "  values. To apply an activation to y_pred, use output_transform as shown below:\n",
    "\n",
    "  .. code-block:: python\n",
    "\n",
    "      def activated_output_transform(output):\n",
    "          y_pred, y = output\n",
    "          y_pred = torch.sigmoid(y_pred)\n",
    "          return y_pred, y\n",
    "\n",
    "      roc_auc = ROC_AUC(activated_output_transform)\n",
    "\n",
    "  \"\"\"\n",
    "    def __init__(self,\n",
    "                 output_transform=lambda x: x,\n",
    "                 check_compute_fn: bool = False):\n",
    "#         print(output_transform)\n",
    "        super(ROC_AUC_perClass,\n",
    "              self).__init__(roc_auc_perClass_compute_fn,\n",
    "                             output_transform=output_transform,\n",
    "                             check_compute_fn=check_compute_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def conv_output_shape(h_w, kernel_size=1, stride=1, pad=0, dilation=1):\n",
    "    \"\"\"\n",
    "  Utility function for computing output of convolutions\n",
    "  takes a tuple of (h,w) and returns a tuple of (h,w)\n",
    "  \"\"\"\n",
    "    from math import floor\n",
    "    if type(kernel_size) is not tuple:\n",
    "        kernel_size = (kernel_size, kernel_size)\n",
    "    h = floor(((h_w[0] + (2 * pad) - (dilation *\n",
    "                                      (kernel_size[0] - 1)) - 1) / stride) + 1)\n",
    "    w = floor(((h_w[1] + (2 * pad) - (dilation *\n",
    "                                      (kernel_size[1] - 1)) - 1) / stride) + 1)\n",
    "    return h, w\n",
    "\n",
    "# mel.shape,an_x.shape,X_train.shape\n",
    "class testModel(nn.Module):\n",
    "    '''A simple model for testing by overfitting.\n",
    "    '''\n",
    "    def __init__(self, out_channels, h_w, kernel_size, FLAT=False,output_shape=(10,)):\n",
    "        # h_w: height will be always one since we use 1d convolution \n",
    "        super(testModel, self).__init__()\n",
    "        self.out_channels = out_channels\n",
    "        #### CONV\n",
    "        self.conv1 = nn.Conv1d(in_channels=1, # depth of image == depth of filters\n",
    "                               out_channels=self.out_channels, # number of filters \n",
    "                               kernel_size=kernel_size, # size of the filters/kernels\n",
    "                               padding=1)\n",
    "\n",
    "        self.conv1_shape = conv_output_shape(h_w, kernel_size=kernel_size, stride=1, pad=1, dilation=1)\n",
    "        # conv is 1d\n",
    "        self.conv1_shape = (1,self.conv1_shape[1])\n",
    "        \n",
    "        self.fc1 = nn.Linear(self.out_channels * self.conv1_shape[0] *self.conv1_shape[1], 64)  # 100\n",
    "\n",
    "        self.fc2 = nn.Linear(64,output_shape[0])\n",
    "        \n",
    "    def forward(self, x):\n",
    "#         x = x.reshape(1,)\n",
    "#         print(x.shape) #  50,1,108800 (850*128)\n",
    "        x = F.relu(self.conv1(x))\n",
    "#         x = self.pool(x)\n",
    "        # x = self.drop(x)\n",
    "#         print(x.shape)# 58, 2, 108801\n",
    "#         print(self.conv1_shape)\n",
    "#         print(x.shape)\n",
    "        x = x.view(-1, self.out_channels * self.conv1_shape[0] *self.conv1_shape[1])\n",
    "        # batch_norm is missing\n",
    "        x = F.relu((self.fc1(x)))\n",
    "        x = (self.fc2(x))\n",
    "\n",
    "#         x = self.drop(x)\n",
    "\n",
    "#         x = self.fc4(x)\n",
    "#         x = torch.sigmoid(x)\n",
    "#                 x = F.log_softmax(x,dim=1)\n",
    "        return x\n",
    "\n",
    "# test\n",
    "# input_shape=(1,(938*128))\n",
    "# output_shape=(10,)\n",
    "# testModel_ins=adam(out_channels=2,h_w=input_shape,kernel_size=2,output_shape=output_shape)\n",
    "# # a.conv1.weight\n",
    "# a_out=testModel_ins(torch.ones((3,1,input_shape[1])))\n",
    "\n",
    "# a_out_correct=torch.zeros(a_out.shape)\n",
    "# a_out_correct[0][:]=1\n",
    "# a_out_correct\n",
    "# a_out.detach().numpy()\n",
    "\n",
    "# torch.exp(a_out),a_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# for y_true_perClass_Index in a_out_correct.shape[1]:\n",
    "#     print(y_true_perClass_Index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from ignite.contrib.metrics import ROC_AUC\n",
    "# from nna.exp.metrics import ROC_AUC_perClass\n",
    "def activated_output_transform(output):\n",
    "    y_pred, y = output\n",
    "#     y_pred = torch.exp(y_pred)\n",
    "    return y_pred, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# asd=ROC_AUC_perClass(activated_output_transform)\n",
    "# asd.update((a_out,a_out_correct))\n",
    "# asd.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "transformCompose = transforms.Compose([\n",
    "#     pitch,\n",
    "#     strech,\n",
    "#     shift,\n",
    "#     noise,\n",
    "    toTensor,\n",
    "])\n",
    "\n",
    "\n",
    "sound_datasets = {\n",
    "    phase: runutils.audioDataset(XY[0], XY[1], transform=transformCompose)\n",
    "    for phase, XY in\n",
    "    zip(['train', 'val', 'test'],\n",
    "        [[X_train, y_train], [X_val, y_val], [X_test, y_test]])\n",
    "}\n",
    "\n",
    "dataloaders = {\n",
    "    x: torch.utils.data.DataLoader(sound_datasets[x], **params)\n",
    "    for x in ['train', 'val', 'test']\n",
    "}\n",
    "\n",
    "# this will change\n",
    "h_w = [128, 938]\n",
    "kernel_size = (4, 4)\n",
    "# if config['CNNLayer_count'] == 1:\n",
    "#     model = modelArchs.NetCNN1(config['CNN_filters_1'], h_w,\n",
    "#                                 kernel_size).float().to(device)\n",
    "\n",
    "# if config['CNNLayer_count'] == 2:\n",
    "#     model = modelArchs.NetCNN2(config['CNN_filters_1'], config.CNN_filters_2,\n",
    "#                                 h_w, kernel_size,\n",
    "#                                 kernel_size).float().to(device)\n",
    "\n",
    "#simpler model\n",
    "\n",
    "output_shape=(CATEGORY_COUNT,)\n",
    "model = testModel(out_channels=3,h_w=(1,h_w[0]*h_w[1]),kernel_size=kernel_size[0]*kernel_size[0],output_shape=output_shape)\n",
    "model.float().to(device)\n",
    "\n",
    "# device is defined before\n",
    "\n",
    "model.float().to(device)  # Move model before creating optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(),\n",
    "#                                 weight_decay=config['weight_decay'],\n",
    "                             )\n",
    "\n",
    "\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "# statHistory={'valLoss':[],'trainLoss':[],'trainAUC':[],'valAUC':[]}\n",
    "\n",
    "metrics = {\n",
    "    'loss': Loss(criterion),  # 'accuracy': Accuracy(),\n",
    "#     'ROC_AUC': ROC_AUC(runutils.activated_output_transform),\n",
    "    'ROC_AUC': ROC_AUC_perClass(activated_output_transform),\n",
    "\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "testModel(\n",
       "  (conv1): Conv1d(1, 3, kernel_size=(16,), stride=(1,), padding=(1,))\n",
       "  (fc1): Linear(in_features=360153, out_features=64, bias=True)\n",
       "  (fc2): Linear(in_features=64, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[ 0.1959, -0.1027, -0.0463, -0.1706,  0.1654, -0.1373,  0.0052,\n",
       "          -0.0930,  0.0941,  0.0957,  0.1334, -0.1755,  0.0965, -0.2100,\n",
       "          -0.1991, -0.0426]],\n",
       "\n",
       "        [[-0.2010,  0.0890,  0.0242, -0.2161, -0.0938,  0.2365, -0.1185,\n",
       "           0.1238,  0.2354,  0.0251, -0.1496, -0.0872,  0.1613,  0.1720,\n",
       "          -0.1681, -0.1265]],\n",
       "\n",
       "        [[ 0.2311,  0.1814, -0.0466,  0.1559, -0.0198,  0.2093, -0.1870,\n",
       "          -0.0874,  0.1201, -0.0762, -0.0304,  0.0805, -0.0199,  0.1263,\n",
       "          -0.1461,  0.0588]]], device='cuda:1', requires_grad=True)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.conv1.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv1d(1, 3, kernel_size=(16,), stride=(1,), padding=(1,))"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.conv1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-6.6027e-04,  8.9407e-04, -2.3156e-04,  ..., -1.4777e-03,\n",
       "          1.1756e-03, -7.9286e-04],\n",
       "        [ 1.4804e-03, -2.7904e-04,  1.6250e-03,  ..., -1.0277e-03,\n",
       "          1.3384e-03,  1.5043e-03],\n",
       "        [-1.1536e-03, -1.3624e-03, -3.8383e-04,  ...,  7.3138e-04,\n",
       "         -1.2281e-03, -2.1300e-04],\n",
       "        ...,\n",
       "        [-5.2222e-05,  8.9399e-04, -2.5988e-04,  ..., -4.2683e-04,\n",
       "          1.2761e-03, -1.3676e-03],\n",
       "        [ 1.9569e-04,  3.4949e-04, -9.6435e-04,  ..., -8.7473e-04,\n",
       "          4.1795e-04,  6.8093e-04],\n",
       "        [-1.8521e-04,  1.1838e-03, -2.2278e-04,  ..., -4.0227e-04,\n",
       "         -1.4945e-03, -5.9660e-04]], device='cuda:1', requires_grad=True)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fc1.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ready ?\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:o9j2zy12) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 153177<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('ready ?')\n",
    "runutils.run(model, dataloaders, optimizer, criterion, metrics, device,config, runconfigs.PROJECT_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fc1.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "example_input = (torch.ones((1,1,h_w[0]*h_w[1]))*-32767)\n",
    "example_input = example_input.float().to(device)\n",
    "out=model(example_input)\n",
    "out\n",
    "\n",
    "activated_output_transform((out,out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.Tensor([10,10,10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(model.fc1.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = nn.Sigmoid()\n",
    "\n",
    "loss = nn.BCEWithLogitsLoss()\n",
    "input = torch.tensor([100.0,100,100], requires_grad=True)\n",
    "# target = torch.empty(3).random_(2)\n",
    "target = torch.ones(3)\n",
    "output = loss((input), target)\n",
    "input,target,output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "soundenv3",
   "language": "python",
   "name": "soundenv3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

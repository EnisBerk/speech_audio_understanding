{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('../../'))\n",
    "print(module_path)\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydub import AudioSegment\n",
    "import soundfile as sf\n",
    "from params import EXCERPT_LENGTH,INPUT_DIR_PARENT,OUTPUT_DIR\n",
    "# sys.path.insert(0, './models/audioset')\n",
    "\n",
    "# from vggish_params import EXAMPLE_HOP_SECONDS\n",
    "from pre_process_func import cal_sample_size,iterate_for_waveform_to_examples\n",
    "import os \n",
    "import math\n",
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/enis/conda/envs/speechEnv/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/scratch/enis/conda/envs/speechEnv/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/scratch/enis/conda/envs/speechEnv/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/scratch/enis/conda/envs/speechEnv/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/scratch/enis/conda/envs/speechEnv/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/scratch/enis/conda/envs/speechEnv/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "sys.path.insert(0, './models/audioset')\n",
    "from models.audioset import vggish_slim\n",
    "from models.audioset import vggish_params\n",
    "from models.audioset import vggish_input\n",
    "from models.audioset import vggish_postprocess\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "src=\"/tank/data/nna/real/11A/\"\n",
    "file=\"/tank/data/nna/real/11A/S4A10276_20190514_153000.flac\"\n",
    "# flac_file = AudioSegment.from_file(file, \"flac\")\n",
    "# ffmpeg -i /tank/data/nna/real/11A/S4A10276_20190514_153000.flac -map 0 -c copy -f segment -segment_time \"02:00:00\" ./output_%03d.flac\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "flacs=os.listdir(\"/tank/data/nna/real/11A\")\n",
    "wav_data, sr = sf.read(Path(src+flac[0]),dtype='int16')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import soundfile as sf\n",
    "wav_data, sr = sf.read(Path(\"/scratch/enis/data/nna/real/39B/S4A10262_20190611_101602_segments/output000.flac\"),dtype='int16')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.10.2'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4438 480000 384000 138960.0\n",
      "213024000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-c42b8f6544f5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0msound\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0miterate_for_waveform_to_examples2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwav_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msound\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-ef8e709b7d7f>\u001b[0m in \u001b[0;36miterate_for_waveform_to_examples2\u001b[0;34m(wav_data, sr)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;31m# left data is smaller than 22712, we cannot pre-process\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;31m# if smaller than 42998, will be 0 anyway\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0ma_sound\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mvggish_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwaveform_to_examples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwav_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moffset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0msound\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0ma_sound\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0ma_sound\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mcount\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0ma_sound\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/nna/models/audioset/vggish_input.py\u001b[0m in \u001b[0;36mwaveform_to_examples\u001b[0;34m(data, sample_rate)\u001b[0m\n\u001b[1;32m     46\u001b[0m   \u001b[0;31m# Resample to the rate assumed by VGGish.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0msample_rate\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mvggish_params\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSAMPLE_RATE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresampy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvggish_params\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSAMPLE_RATE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m   \u001b[0;31m# Compute log mel spectrogram features.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/enis/conda/envs/speechEnv/lib/python3.7/site-packages/resampy/core.py\u001b[0m in \u001b[0;36mresample\u001b[0;34m(x, sr_orig, sr_new, axis, filter, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[0mx_2d\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mswapaxes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[0my_2d\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mswapaxes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m     \u001b[0mresample_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_2d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_2d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_ratio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterp_win\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterp_delta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecision\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for flac in flacs[1:2]:\n",
    "    wav_data, sr = sf.read(\"/scratch/enis/data/nna/real/27A/S4A10251_20190504_000000_segments/output000.flac\",dtype='int16')\n",
    "\n",
    "    sample_size,offset,remainder_wav_data,lower_limit=cal_sample_size(wav_data,sr)\n",
    "    print(sample_size,offset,remainder_wav_data,lower_limit)\n",
    "    \n",
    "    \n",
    "    sound=iterate_for_waveform_to_examples2(wav_data,sr)\n",
    "    print(sound.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "443.8"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wav_data)/offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_for_waveform_to_examples2(wav_data,sr):\n",
    "    \"\"\"Wrapper for waveform_to_examples from models/audioset/vggish_input.py\n",
    "\n",
    "        Iterate over data with 10 seconds batches, so waveform_to_examples produces\n",
    "        stable results (equal size)\n",
    "        read **(16/06/2019)** at Project_logs.md for explanations.\n",
    "\n",
    "        Args:\n",
    "            wav_data (numpy.array): audio data in wav format\n",
    "            sr (int): sampling rate of the audio\n",
    "\n",
    "        Returns:\n",
    "            See waveform_to_examples.\n",
    "    \"\"\"\n",
    "    sample_size,offset,remainder_wav_data,lower_limit=cal_sample_size(wav_data,sr)\n",
    "    # in this loop wav_data jumps offset elements and sound jumps EXCERPT_LENGTH*2\n",
    "    # because offset number of raw data turns into EXCERPT_LENGTH*2 pre-processed\n",
    "    sound=np.zeros((sample_size,96,64),dtype=np.float32)\n",
    "    count=0\n",
    "    print(len(wav_data))\n",
    "    for i in range(0,len(wav_data),offset):\n",
    "    #this is when wav_data%offset!=0\n",
    "        # numpy indexing handles bigger indexes\n",
    "        # i+offset>len(wav_data) means that we are on the last loop\n",
    "        # then if there is enough remaind data, process it otherwise not\n",
    "        if i+offset>len(wav_data) and remainder_wav_data<lower_limit:\n",
    "            continue\n",
    "        # left data is smaller than 22712, we cannot pre-process\n",
    "        # if smaller than 42998, will be 0 anyway\n",
    "        a_sound= vggish_input.waveform_to_examples(wav_data[i:i+(offset)], sr)\n",
    "        sound[count:(count+a_sound.shape[0]),:,:]=a_sound[:,:,:]\n",
    "        count+=a_sound.shape[0]\n",
    "    return sound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "213024000 (213024000, 2) (4439, 96, 64)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(len(data),data.shape,sound.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vggfile=\"/scratch/enis/data/nna/NUI_DATA/01 Itkillik/August 2016/ITKILLIK1_20160727_135107_vgg/ITKILLIK1_20160727_135107_rawembeddings049.npy\"\n",
    "\n",
    "# vggnp=np.load(vggfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vggfile=\"/scratch/enis/data/nna/NUI_DATA/01 Itkillik/August 2016/ITKILLIK1_20160727_135107_vgg/ITKILLIK1_20160727_135107_rawembeddings048.npy\"\n",
    "\n",
    "# vggnp8=np.load(vggfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vggnp[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vggnp8[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vggfile=\"/scratch/enis/data/nna/NUI_DATA/01 Itkillik/August 2016/ITKILLIK1_20160731_171332_vgg/ITKILLIK1_20160731_171332_rawembeddings048.npy\"\n",
    "# vggnp8_2=np.load(vggfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vggnp8_2[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# name=\"ITKILLIK1_20160825_132756\"\n",
    "# vggfile=\"/scratch/enis/data/nna/NUI_DATA/01 Itkillik/August 2016/{}_vgg/{}_rawembeddings049.npy\".format(name,name)\n",
    "# vggnp9_2=np.load(vggfile)\n",
    "# vggnp9_2[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vggnp9_2[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46799\n",
      "shape is 1 starting from  46800\n",
      "92879\n",
      "shape is 2 starting from  92880\n",
      "138959\n",
      "shape is 3 starting from  138960\n",
      "185039\n",
      "shape is 4 starting from  185040\n",
      "231119\n",
      "shape is 5 starting from  231120\n",
      "277199\n",
      "shape is 6 starting from  277200\n",
      "323279\n",
      "shape is 7 starting from  323280\n",
      "369359\n",
      "shape is 8 starting from  369360\n",
      "415439\n",
      "shape is 9 starting from  415440\n",
      "461519\n",
      "shape is 10 starting from  461520\n"
     ]
    }
   ],
   "source": [
    "wav_data=data[:]\n",
    "sr=samplerate\n",
    "vggish_params.EXAMPLE_HOP_SECONDS=0.96\n",
    "\n",
    "initial=-1\n",
    "# 22712 0 starts ?? 22712\n",
    "# 42998 1 starts ?? 42998\n",
    "# 63284 2 starys ?? 85334\n",
    "        # 3        #  ?? 127734\n",
    "    # 63284-42998 = 20286 !    ?? 42336\n",
    "    #               297014\n",
    "#       8  starts  339350\n",
    "#       9 starts   381686    \n",
    "#       10 starts  424022   \n",
    "#       11 starts  466358 \n",
    "# 22712+(seconds*20286)=len_wav_data\n",
    "\n",
    "for index in range(1,11):\n",
    "    for i in range(int((sr*0.96*index)+(719)),(EXCERPT_LENGTH+1)*sr,1):\n",
    "        #   print(i)\n",
    "        a_sound= vggish_input.waveform_to_examples(wav_data[0:i], sr)\n",
    "        #   print(i,\" \",a_sound.shape)\n",
    "        if a_sound.shape[0]==index:\n",
    "            print(\"shape is\",a_sound.shape[0],\"starting from \",i)\n",
    "\n",
    "            initial=a_sound.shape[0]\n",
    "            break\n",
    "        if i%1==0:\n",
    "            print(i)\n",
    "#   break\n",
    "# for 0.96\n",
    "# shape is 1 starting from  42998\n",
    "# shape is 2 starting from  85334\n",
    "# shape is 3 starting from  127670\n",
    "# shape is 4 starting from  170006\n",
    "# shape is 5 starting from  212342\n",
    "# shape is 6 starting from  254678\n",
    "# shape is 7 starting from  297014\n",
    "# shape is 8 starting from  339350\n",
    "# shape is 9 starting from  381686\n",
    "# shape is 10 starting from  424022\n",
    "# shape is 11 starting from  466358\n",
    "\n",
    "# for flac with 48000 sampling rate\n",
    "# shape is 1 starting from  46800\n",
    "# shape is 2 starting from  92880\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.148260593414307\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import resampy\n",
    "\n",
    "s=time.time()\n",
    "if len(data.shape) > 1:\n",
    "    datas1 = np.mean(data[:213024000], axis=1)\n",
    "datas = resampy.resample(datas1, 41000, 16000)\n",
    "\n",
    "\n",
    "e=time.time()\n",
    "t1=(e-s)\n",
    "\n",
    "s=time.time()\n",
    "if len(data.shape) > 1:\n",
    "    datas1 = np.mean(data[:4100000], axis=1)\n",
    "datas2 = resampy.resample(datas1, 41000, 16000)\n",
    "\n",
    "\n",
    "e=time.time()\n",
    "t2=(e-s)\n",
    "print((213024000/4100000)*t2/t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from params import LOGS_FILE\n",
    "import pre_process_func\n",
    "from pathlib import Path\n",
    "from subprocess import Popen, PIPE\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "TEST_DIR=Path(\"tests/\")\n",
    "EXAMPLE_MODELS_DIR=TEST_DIR / \"example_models/\"\n",
    "EXAMPLE_OUTPUT_DIR= TEST_DIR / \"example_outputs\"\n",
    "OUTPUT_DIR=Path(\"aggregates\")\n",
    "\n",
    "root_dir=\"/tank/data/nna/real/\"\n",
    "# input_path_list=[root_dir+\"tests/data/3hours30min.mp3\",]\n",
    "\n",
    "input_path_list=[root_dir+\"39B/S4A10262_20190611_101602.flac\",]\n",
    "\n",
    "pre_process_func.parallel_pre_process(input_path_list,\n",
    "                                        output_dir=\"./tests/data/output\",\n",
    "                                        input_dir_parent=root_dir,\n",
    "                                        cpu_count=2,\n",
    "                                        segment_len=\"02:00:00\",\n",
    "                                        logs_file_path=LOGS_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "speechEnv",
   "language": "python",
   "name": "speechenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

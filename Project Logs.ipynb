{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process:<br>\n",
    "* (08/3/2019) Understand commmon tools and data structure for Speech and Audio Processsing <br>\n",
    "  * [kaggle_speech.ipynb](./notebooks/kaggle_speech.ipynb) Going through dataset and kernels from [TensorFlow Speech Recognition Challenge](https://www.tensorflow.org/tutorials/sequences/audio_recognition)\n",
    "* (15/3/2019) I used vanilla classifiers for predicting location and recording hour of the sound given VGGish embeddings,highest accuracy results are as following ([notebook](./notebooks/Classify_time&location.ipynb):\n",
    "    * LinearSVC 0.882 for location\n",
    "    * LinearSVC 0.867 for hour of the day\n",
    "* (22/03/2019) I classified embeddings generated with VGGish model using an attention model [original audioset model](https://github.com/qiuqiangkong/audioset_classification). Using post-processed (PCA & whitening) embeddings did not provide any meaningfully classification.  When I used unprocessed embeddings, class assignments were not meaningful. I also tried to cluster embeddings with t-sne and analyze results, however those cluster did not provide so much information as well. [notebook](./notebooks/Audioset_model_inference.ipynb)\n",
    "* (29/03/2019) I created a pipeline that can process audio files in parallel so I can work with big files without a lot of hands on work. [pipeline md file](./src/hpc_pipeline.md)\n",
    "* (05/04/2019) Since I could not find meaningful clusters on sampled data, I took 24 hours continuous recordings, and analyzed those. You can see t-sne visualization of those . Every point represents 10 seconds of recording (embeddings are averaged). Embeddings [averaged_embeddings_1](./vis/averaged_embeddings_1.html) or post-processed embeddings [averaged_post_1](./vis/averaged_post_1.html). However there is not any obvious clustering. \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python speechEnv",
   "language": "python",
   "name": "speechenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

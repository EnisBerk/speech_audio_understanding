{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import sys\n",
    "\n",
    "# module_path = os.path.abspath(os.path.join('../../src'))\n",
    "# print(\"adding following folder to path: \",module_path)\n",
    "# if module_path not in sys.path:\n",
    "#     sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from numpy import linspace\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import datetime\n",
    "from scipy import stats\n",
    "import time\n",
    "import copy\n",
    "\n",
    "\n",
    "import csv \n",
    "from pathlib import Path\n",
    "from os import listdir\n",
    "import pickle\n",
    "\n",
    "from tabulate import tabulate\n",
    "\n",
    "\n",
    "from datetime import datetime,timedelta\n",
    "\n",
    "import itertools\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap, ListedColormap\n",
    "import matplotlib.ticker as ticker\n",
    "import matplotlib.dates as mdates\n",
    "from matplotlib.ticker import NullFormatter\n",
    "from matplotlib.colors import Normalize\n",
    "import matplotlib.pylab as pl\n",
    "from matplotlib.collections import LineCollection\n",
    "\n",
    "from pytz import timezone\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nna.pre_process_func import read_queue\n",
    "from nna.fileUtils import read_file_properties\n",
    "from nna.labeling_utils import load_labels\n",
    "from nna.visUtils import get_cycle,createTimeIndex,file2TableDict,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INPUT_DIR_PARENT = \"/home/data/nna/stinchcomb/NUI_DATA/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "linestyle_str = [\n",
    "     ('solid', 'solid'),      # Same as (0, ()) or '-'\n",
    "     ('dotted', 'dotted'),    # Same as (0, (1, 1)) or '.'\n",
    "     ('dashed', 'dashed'),    # Same as '--'\n",
    "     ('dashdot', 'dashdot'),  # Same as '-.\n",
    "     ('densely dotted',        (0, (1, 1))),\n",
    "     ('densely dashed',        (0, (5, 1))),\n",
    "    ('densely dashdotted',    (0, (3, 1, 1, 1))),\n",
    "     ('densely dashdotdotted', (0, (3, 1, 1, 1, 1, 1)))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataFlder = \"/home/enis/projects/nna/data/\"\n",
    "resultsFlder = \"/home/enis/projects/nna/results/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2name={}\n",
    "id2name[\"_CABLE\"]=\"Cable\"\n",
    "id2name[\"_RUNNINGWATER\"]=\"Running Water\"\n",
    "id2name[\"_INSECT\"]=\"Insect\"\n",
    "id2name[\"_RAIN\"]=\"Rain\"\n",
    "id2name[\"_WATERBIRD\"]=\"Water Bird\"\n",
    "id2name[\"_WIND\"]=\"Wind\"\n",
    "id2name[\"_SONGBIRD\"]=\"Songbird\"\n",
    "id2name[\"_AIRCRAFT\"]=\"Aircraft\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_properties_df=pd.read_pickle(\"../../data/stinchcomb_dataV1.pkl\")\n",
    "file_properties_df=pd.read_pickle(\"../../data/prudhoeAndAnwr4photoExp_dataV1.pkl\")\n",
    "\n",
    "#important to keep them in order\n",
    "file_properties_df.sort_values(by=['timestamp'],inplace=True)\n",
    "\n",
    "# delete older than 2016\n",
    "fromtime=datetime(2016, 1, 1, 0)\n",
    "file_properties_df=file_properties_df[file_properties_df.timestamp>=fromtime]\n",
    "all_areas=sorted(pd.unique(file_properties_df.site_id.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# file_properties_df=pd.read_pickle(\"../../data/stinchcomb_dataV1.pkl\")\n",
    "# file_properties_df2=pd.read_pickle(\"../../data/realdata_v2No_stinchcomb.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# asd=pd.concat([file_properties_df,file_properties_df2])\n",
    "# asd.sort_values(by=['timestamp'],inplace=True)\n",
    "\n",
    "# asd.to_pickle(\"../../data/allFields_dataV3.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !ls ../../data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_properties_df[file_properties_df.site_id==\"47\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARAMS\n",
    "# FREQS to reduce results \n",
    "# freq=\"30min\"\n",
    "# freq=\"2H\"\n",
    "freq=\"270min\"\n",
    "# freq=\"135min\"\n",
    "# freq=\"continous\"\n",
    "\n",
    "\n",
    "\n",
    "# possible places to pick\n",
    "# sorted(pd.unique(file_properties_df.site_id.values))\n",
    "# areas to be visualized\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# globalindex,all_start,all_end=createTimeIndex(selected_areas,file_properties_df,freq)\n",
    "\n",
    "# selected_tag_name=\"_SONGBIRD\"\n",
    "\n",
    "\n",
    "\n",
    "# weather_cols=[]\n",
    "\n",
    "visFilePath=\"/home/enis/projects/nna/results/vis/PrudhoeAnwrV1/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\" Duration of selected data period:\",(all_end-all_start).days,\"days\")\n",
    "# print(\" Starts: {} \\n Ends:   {}\".format(all_start,all_end))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# globalindex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_properties_df[file_properties_df.site_id==\"31\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2019-09-09 14:18:02')"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "file_properties_df.iloc[-1][\"timestamp\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_properties_df[file_properties_df.site_id==selected_area]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nna.fileUtils import standardPathStyle,list_files\n",
    "from nna.visUtils import loadResults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file2TableDict(selected_areas,model_tag_names,globalindex,globalcolumns,\n",
    "                    file_properties_df,freq,dataFreq=\"10S\",dataThreshold=0.5,\n",
    "                    channel=1,gathered_results_perTag=None,\n",
    "                    result_path=None,fileNameAddon=\"\",prob2binaryFlag=True):\n",
    "    # using gathered_results_perTag dictionary or  result_path to create\n",
    "    # a pandas dataframe for visualizations\n",
    "\n",
    "    # dataFreq is sampling frequency of the data,\n",
    "    #most of the time we have predictions for each 10 second\n",
    "\n",
    "    df_dict={key: None for (key) in selected_areas}\n",
    "    no_result_paths=[]\n",
    "\n",
    "    #we need to load it from files\n",
    "    if gathered_results_perTag==None and (result_path==None):\n",
    "        print(\"ERROR: gathered_results_perTag or (result_path and subDirectoryAddon )should be defined\")\n",
    "        return (None,None)\n",
    "\n",
    "\n",
    "    for i,area in enumerate(selected_areas):\n",
    "        df_sums = pd.DataFrame(index=globalindex, columns=globalcolumns).fillna(0)\n",
    "        df_count = pd.DataFrame(index=globalindex, columns=globalcolumns).fillna(0)\n",
    "\n",
    "        for modelTagName in model_tag_names:\n",
    "    #         for afile in selected_areas_dict[area]:\n",
    "            area_filtered=file_properties_df[file_properties_df.site_id==area]\n",
    "            for afile,row in area_filtered.iterrows():\n",
    "        #         data=gathered_results[afile][0]\n",
    "                afile=Path(afile)\n",
    "                # we either load data from multiple files or from single one\n",
    "                if gathered_results_perTag==None:\n",
    "                    # TODO, make _FCmodel variable\n",
    "                    checkFolder=standardPathStyle(result_path,row,subDirectoryAddon=modelTagName\n",
    "                                        ,fileNameAddon=fileNameAddon)\n",
    "                    allSegments = list_files(str(checkFolder)+\"/\")\n",
    "                    allSegments.sort()\n",
    "                    if not allSegments:\n",
    "                        data=np.empty(0)\n",
    "                    else:\n",
    "                        data=loadResults(allSegments,prob2binaryFlag=prob2binaryFlag,\n",
    "                                        threshold=dataThreshold,channel=channel)\n",
    "                        # gathered_results[file]=result[:]\n",
    "                else:\n",
    "                    data=gathered_results_perTag[modelTagName].get(afile,np.empty(0))[:]\n",
    "                    if data.size!=0 and prob2binaryFlag==True:\n",
    "                        data=prob2binary(data,threshold=0.5,channel=channel)\n",
    "\n",
    "                if data.size==0:\n",
    "                    no_result_paths.append(afile)\n",
    "                    continue\n",
    "\n",
    "                start=file_properties_df.loc[afile][\"timestamp\"]\n",
    "                end =start+timedelta(seconds=(10*(len(data)-1)))\n",
    "                index = pd.date_range(start,end, freq=dataFreq)\n",
    "                df_afile=pd.DataFrame(data,index=index,columns=[modelTagName])\n",
    "                # df_afile_grouped = df_afile.groupby([pd.Grouper(freq=freq)])\n",
    "                # counts=df_afile_grouped.count()\n",
    "                # sums=df_afile_grouped.sum()\n",
    "                globalindexStart=globalindex.searchsorted(df_afile.index[0])\n",
    "                globalindexStart= 0 if globalindexStart==0 else globalindexStart-1\n",
    "                globalindexEnd=globalindex.searchsorted(df_afile.index[-1])\n",
    "                globalindexEnd= globalindexEnd+1 if globalindexEnd==globalindexStart else globalindexEnd\n",
    "#                 print(globalindexStart)\n",
    "                theBins=pd.cut(df_afile.index,globalindex[globalindexStart:globalindexEnd+1])\n",
    "                # theBins=pd.cut(df_afile.index,globalindex)\n",
    "                df_afileGrouped=df_afile.groupby(theBins)\n",
    "                sums=df_afileGrouped.agg(\"sum\")\n",
    "                counts=df_afileGrouped.agg(\"count\")\n",
    "                sums.set_index(sums.index.categories.left,inplace=True)\n",
    "                counts.set_index(counts.index.categories.left,inplace=True)\n",
    "\n",
    "                df_count=df_count.add(counts, fill_value=0) #df_count.update(counts)\n",
    "                df_sums=df_sums.add(sums, fill_value=0) #df_sums.update(sums)\n",
    "\n",
    "        df_dict[area]=(df_count.copy(),df_sums.copy())\n",
    "\n",
    "    return df_dict,no_result_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 0\n",
      "60 number of files do not have results\n",
      "407 number of files do not have results\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/enis/conda/envs/speechEnv/lib/python3.7/site-packages/matplotlib/colors.py:527: RuntimeWarning: invalid value encountered in less\n",
      "  xa[xa < 0] = -1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 1\n",
      "51 number of files do not have results\n",
      "379 number of files do not have results\n",
      "13 2\n",
      "47 number of files do not have results\n",
      "495 number of files do not have results\n",
      "14 3\n",
      "56 number of files do not have results\n",
      "494 number of files do not have results\n",
      "15 4\n",
      "35 number of files do not have results\n",
      "374 number of files do not have results\n",
      "16 5\n",
      "10 number of files do not have results\n",
      "146 number of files do not have results\n",
      "17 6\n",
      "25 number of files do not have results\n",
      "317 number of files do not have results\n",
      "18 7\n",
      "10 number of files do not have results\n",
      "143 number of files do not have results\n",
      "19 8\n",
      "19 number of files do not have results\n",
      "296 number of files do not have results\n",
      "20 9\n",
      "27 number of files do not have results\n",
      "430 number of files do not have results\n",
      "21 10\n",
      "18 number of files do not have results\n",
      "440 number of files do not have results\n",
      "22 11\n",
      "16 number of files do not have results\n",
      "386 number of files do not have results\n",
      "23 12\n",
      "19 number of files do not have results\n",
      "343 number of files do not have results\n",
      "24 13\n",
      "9 number of files do not have results\n",
      "445 number of files do not have results\n",
      "25 14\n",
      "8 number of files do not have results\n",
      "449 number of files do not have results\n",
      "26 15\n",
      "13 number of files do not have results\n",
      "412 number of files do not have results\n",
      "27 16\n",
      "334 number of files do not have results\n",
      "28 17\n",
      "367 number of files do not have results\n",
      "29 18\n",
      "436 number of files do not have results\n",
      "30 19\n",
      "448 number of files do not have results\n",
      "31 20\n",
      "356 number of files do not have results\n",
      "32 21\n",
      "440 number of files do not have results\n",
      "33 22\n",
      "34 23\n",
      "323 number of files do not have results\n",
      "35 24\n",
      "392 number of files do not have results\n",
      "36 25\n",
      "68 number of files do not have results\n",
      "37 26\n",
      "58 number of files do not have results\n",
      "38 27\n",
      "226 number of files do not have results\n",
      "39 28\n",
      "380 number of files do not have results\n",
      "40 29\n",
      "415 number of files do not have results\n",
      "41 30\n",
      "4 number of files do not have results\n",
      "400 number of files do not have results\n",
      "42 31\n",
      "251 number of files do not have results\n",
      "43 32\n",
      "382 number of files do not have results\n",
      "44 33\n",
      "411 number of files do not have results\n",
      "45 34\n",
      "46 35\n",
      "462 number of files do not have results\n",
      "47 36\n",
      "299 number of files do not have results\n",
      "48 37\n",
      "416 number of files do not have results\n",
      "49 38\n",
      "432 number of files do not have results\n",
      "50 39\n",
      "465 number of files do not have results\n",
      "CPU times: user 1h 58min 50s, sys: 4min 46s, total: 2h 3min 36s\n",
      "Wall time: 5h 21min 37s\n"
     ]
    }
   ],
   "source": [
    "result_path=\"/scratch/enis/data/nna/real/\"\n",
    "model_tag_names=[\"CABLE\",\"RUNNINGWATER\",\"INSECT\", \"RAIN\", \"WATERBIRD\", \"WIND\", \"SONGBIRD\", \"AIRCRAFT\"]\n",
    "def main():\n",
    "    cmap = pl.cm.tab10\n",
    "    aCmap = cmap\n",
    "    my_cmaps = addNormalDistAlpha(aCmap)\n",
    "    for selected_area in all_areas[:]:\n",
    "        print(selected_area,all_areas.index(selected_area))\n",
    "        nna.visutils.vis_preds_with_clipping(selected_area, file_properties_df, freq,model_tag_names, my_cmaps)\n",
    "main()\n",
    "# try:\n",
    "#     main()\n",
    "#     !conda run -n speechEnv python /home/enis/projects/nna/src/slack_message.py -m \"continous figures ready \"\n",
    "# except Exception as ex:\n",
    "#     print(ex)\n",
    "#     !conda run -n speechEnv python /home/enis/projects/nna/src/slack_message.py -m \"continous figures failed\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def VisPredsWClipping(selected_area,file_properties_df,freq,model_tag_names,my_cmaps):\n",
    "    selected_areas=[selected_area,]\n",
    "    # file length based time index\n",
    "    if freq==\"continous\":\n",
    "        fileTimeIndexSeries = getTimeIndexPerFile(selected_area,file_properties_df,freq)\n",
    "        globalindex = fileTimeIndexSeries\n",
    "    #fixed freq based  time index\n",
    "    else:\n",
    "        globalindex,all_start,all_end = createTimeIndex(selected_areas,file_properties_df,freq)\n",
    "\n",
    "    selected_tag_name=[\"_\"+i for i in model_tag_names]\n",
    "    globalcolumns=selected_tag_name #selected_areas+weather_cols\n",
    "    \n",
    "#     print(globalindex)\n",
    "    df_dict,no_result_paths = file2TableDict(selected_areas,selected_tag_name,globalindex,\n",
    "                                             globalcolumns,file_properties_df,freq,dataFreq=\"10S\",\n",
    "                                             result_path=result_path,prob2binaryFlag=False)\n",
    "    if len(no_result_paths)!=0:\n",
    "        print(\"{} number of files do not have results\".format(len(no_result_paths)))\n",
    "#         print(no_result_paths[:1])\n",
    "#         print(no_result_paths[-1:])\n",
    "\n",
    "    \n",
    "    regionName=file_properties_df[file_properties_df.site_id==selected_area][:1].region[0]\n",
    "\n",
    "    # we are not using this for visualizations\n",
    "    # df_dict_reverse=reverseTableDict(selected_areas,df_dict,model_tag_names)\n",
    "    df_count,df_sums = df_dict[selected_area]\n",
    "\n",
    "    df_freq=df_sums/df_count\n",
    "    # del df_freq['UMIAT']\n",
    "    df_freq=df_freq*100\n",
    "    \n",
    "    ########     LOAD Clipping     #########\n",
    "    clippingResultsPath=dataFlder+\"clipping_results_old/\"\n",
    "    selected_tag_name=\"Clipping\"\n",
    "#     model_tag_names=[selected_tag_name]\n",
    "    globalcolumns=[selected_tag_name] #selected_areas+weather_cols\n",
    "\n",
    "    gathered_results_perTag = loadClipping2Dict(clippingResultsPath,selected_areas,selected_tag_name)\n",
    "\n",
    "    df_dict_clipping,no_result_paths = file2TableDict(selected_areas,globalcolumns,globalindex,globalcolumns,\n",
    "                                    file_properties_df,freq,dataFreq=\"10S\",dataThreshold=0.01,channel=2,\n",
    "                                    gathered_results_perTag=gathered_results_perTag,result_path=None)\n",
    "\n",
    "    df_count_clipping,df_sums_clipping = df_dict_clipping[selected_area]\n",
    "    df_freq_clipping = df_sums_clipping / df_count_clipping\n",
    "    df_freq_clipping = df_freq_clipping * 100\n",
    "\n",
    "    ### add Clipping data to predictions \n",
    "\n",
    "    df_freq = pd.concat([df_freq, df_freq_clipping], axis=1, sort=False)\n",
    "    if len(no_result_paths)!=0:\n",
    "        print(\"{} number of files do not have results\".format(len(no_result_paths)))\n",
    "\n",
    "    ### Divide data into months\n",
    "\n",
    "    cord_list=[(i,(0,0)) for i in df_freq.columns]\n",
    "    \n",
    "    monthsTime=pd.unique(df_freq.index.strftime(\"%Y-%m-01\"))\n",
    "    monthsTime=[pd.Timestamp(i) for i in monthsTime]\n",
    "    \n",
    "    monthsTimeStr=[\"{}-{}\".format(month.year,month.month) for month in monthsTime]\n",
    "    months=[df_freq.loc[month:month] for month in monthsTimeStr]\n",
    "    ##### align all months\n",
    "    for i,month in enumerate(months):\n",
    "        months[i]=month.rename(index=lambda x: x.replace(month=7,year=2019))\n",
    "\n",
    "    uniqueYears=np.unique([month.year for month in monthsTime])\n",
    "    for year in uniqueYears:\n",
    "        monthsInAYear=[months[i] for i,month in enumerate(monthsTime) if month.year==year]\n",
    "        monthsTimeInAYear=[monthsTime[i] for i,month in enumerate(monthsTime) if month.year==year]\n",
    "        createFigure(selected_area,monthsInAYear,monthsTimeInAYear,my_cmaps,cord_list,visFilePath,regionName,year,)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTimeIndexPerFile(selected_area,file_properties_df,freq,timeDifference=5):\n",
    "    #TODO add sensitivity\n",
    "    rowiterator=file_properties_df[file_properties_df.site_id==selected_area].iterrows()\n",
    "    # use first item as initialization, ahead of for loop\n",
    "    n=next(rowiterator)\n",
    "    start=n[1].timestamp\n",
    "    beginning=start\n",
    "    end=n[1].timestampEnd\n",
    "    fileTimeIndex=[]\n",
    "    IsHead=False\n",
    "#     print(start)\n",
    "    for row in rowiterator:\n",
    "        # if end of previous file not equal to start of the second one\n",
    "        if (row[1].timestamp-end)>timedelta(minutes=timeDifference):\n",
    "            # add previous one to list and make new one the beginning of continous recording\n",
    "            fileTimeIndex.append(beginning)\n",
    "            beginning=row[1].timestamp\n",
    "            IsHead=True\n",
    "    #             pass\n",
    "#             print(\"noteq\",row[1].timestamp-end)\n",
    "    #         print(row[1].timestamp,end)\n",
    "        # if they are equal, they should be in the same bin, so keep going\n",
    "        else:\n",
    "            IsHead=False\n",
    "    #             pass\n",
    "    #         print(\"equal\",row[1].timestampEnd-start)\n",
    "    #         fileTimeIndex.append(start)\n",
    "    #         print(row[1].timestampEnd,start)\n",
    "        start=row[1].timestamp\n",
    "        end=row[1].timestampEnd\n",
    "    # If last one is a head add to the list\n",
    "#     print(IsHead)\n",
    "    if IsHead:\n",
    "        fileTimeIndex.append(beginning)\n",
    "    # add end since last bin border should be bigger than all data\n",
    "    fileTimeIndex.append(end)\n",
    "    fileTimeIndexSeries=pd.Series(fileTimeIndex)\n",
    "    return fileTimeIndexSeries\n",
    "\n",
    "\n",
    "# 10191019101019101910101910191\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addNormalDistAlpha(aCmap):\n",
    "    # Choose colormap\n",
    "    # cmap = pl.cm.tab10\n",
    "    cmap=aCmap\n",
    "    # Get the colormap colors\n",
    "    my_cmap = aCmap(np.arange(aCmap.N))\n",
    "    my_cmaps=[]\n",
    "    for clr in my_cmap:\n",
    "        r,g,b,_=clr\n",
    "        cdict = {'red':   [[0.0,  r, r],\n",
    "                       [1.0,  r, r]],\n",
    "             'green': [[0.0,  g,  g],\n",
    "                       [1.0,  g, g]],\n",
    "             'blue':  [[0.0,  b, b],\n",
    "                       [1.0,  b, b]],\n",
    "             'alpha':  [[0,  0.9, 0.9],\n",
    "                       [1,  0.1, 0.1]]}\n",
    "\n",
    "        newcmp = LinearSegmentedColormap('testCmap', segmentdata=cdict, N=100)\n",
    "        my_cmaps.append(newcmp)\n",
    "    return my_cmaps\n",
    "\n",
    "def loadClipping2Dict(clippingResultsPath,selected_areas,selected_tag_name):\n",
    "    gathered_results_perTag={selected_tag_name:{}}\n",
    "    gathered_results={}\n",
    "    selected_areas_files={}\n",
    "    for i,area in enumerate(selected_areas):\n",
    "        to_be_deleted=[]\n",
    "        fileName=(clippingResultsPath+area+\"_1.pkl\")\n",
    "        resultsDict=np.load(fileName,allow_pickle=True)\n",
    "        resultsDict=resultsDict[()]\n",
    "        gathered_results_perTag[selected_tag_name].update(resultsDict)\n",
    "    return gathered_results_perTag\n",
    "\n",
    "\n",
    "# def loadClipping(clippingInfoFile):\n",
    "    \n",
    "#     clippingInfo=np.load(clippingInfoFile,allow_pickle=True)\n",
    "#     clippingInfo = clippingInfo[()]\n",
    "#     clippingInfo2={}\n",
    "#     clippingInfoArray=[]\n",
    "#     cc=0\n",
    "#     for clipFile,clipping in clippingInfo.items():\n",
    "#         locID=Path(clipFile).stem.split(\"_\")[:2]\n",
    "#         locID = tuple(locID)\n",
    "#         clippingInfo2[locID] = clipping\n",
    "#         if clipping.shape==(1,2):\n",
    "#             clipping = clipping[0]\n",
    "#         elif clipping.shape==(2,2):\n",
    "#             clipping = clipping[0]\n",
    "#         elif clipping.shape==(2,):\n",
    "#             pass\n",
    "#         else:\n",
    "#             print(\"ERROR\",clipping)\n",
    "#         clippingInfoArray.append(clipping)\n",
    "#         clippingInfo2[locID] = clipping\n",
    "\n",
    "#     clippingInfoArray = np.concatenate(clippingInfoArray).reshape(-1,2)\n",
    "#     return clippingInfo2,clippingInfoArray\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def createFigure(selected_area,months,monthsTime,my_cmaps,cord_list,visFilePath,regionName,year):\n",
    "#     plt.rcParams[\"axes.prop_cycle\"] = get_cycle(\"tab10\",N=8)\n",
    "    vmin,vmax=0,100\n",
    "    normalize = Normalize(vmin=vmin, vmax=vmax)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(80,len(months)*9),nrows=len(months),sharex=True, sharey=True,gridspec_kw={'hspace': 0})\n",
    "    ax = np.array(ax).reshape(-1) # subplot returns single element for single row\n",
    "\n",
    "    markers = itertools.cycle((',', '+', '.', 'o', '*')) \n",
    "\n",
    "    weather_colors=[\"firebrick\",\"darkorange\",\"green\",\"seagreen\",\"lightpink\"]\n",
    "\n",
    "    for monthi,month in enumerate(months):\n",
    "        # for col in df_freq.columns:\n",
    "        for i,(col,(lat,long)) in enumerate(cord_list):\n",
    "#             if col in weather_cols:\n",
    "#                 index=weather_cols.index(col)\n",
    "#                 ax[monthi].plot_date(month.index.to_pydatetime(), month[col],linestyle=\"-\",marker=\" \",color=weather_colors[index])\n",
    "#             else:\n",
    "#             ax[monthi].plot_date(month.index.to_pydatetime(), month[col],linestyle=\"-\",marker=\" \")\n",
    "            if col==\"Clipping\":\n",
    "                continue\n",
    "            #convert dates to numbers first\n",
    "            inxval = mdates.date2num(month[col].index.to_pydatetime())\n",
    "            points = np.array([inxval, month[col].values]).T.reshape(-1,1,2)\n",
    "            segments = np.concatenate([points[:-1],points[1:]], axis=1)\n",
    "            lc = LineCollection(segments, cmap=my_cmaps[i],norm=normalize, linewidth=3,)\n",
    "            # set color to date values\n",
    "            lc.set_array(month[\"Clipping\"])\n",
    "            # note that you could also set the colors according to y values\n",
    "            # lc.set_array(s.values)\n",
    "            # add collection to axes\n",
    "            ax[monthi].add_collection(lc)\n",
    "#             break\n",
    "\n",
    "    # add legend and set names of the lines\n",
    "    ax[0].legend( labels=[id2name.get(x[0],x[0][1:]) for x in cord_list],loc='upper left', \n",
    "                borderpad=0.2, labelspacing=0.2, fontsize=28, \n",
    "                frameon=True) # frameon=False to remove frame.\n",
    "\n",
    "    # set colours of the lines on the legend\n",
    "    leg = ax[0].get_legend()\n",
    "    for i,(col,(lat,long)) in enumerate(cord_list):\n",
    "        if col==\"Clipping\":\n",
    "            continue\n",
    "        leg.legendHandles[i].set_color(my_cmaps[i](vmin)[:-1])\n",
    "\n",
    "\n",
    "    ax[-1].set_xlabel('Day Number', fontsize=32)\n",
    "\n",
    "#     uniqueYears=pd.unique([month.year for month in monthsTime])\n",
    "#     uniqueYears.size\n",
    "\n",
    "    for i,an_ax in enumerate(ax):    \n",
    "        an_ax.set_ylabel('{}'.format(monthsTime[i].strftime(\"%Y-%B\")),fontsize=48) #, fontweight='black')\n",
    "\n",
    "        locator=mdates.DayLocator()\n",
    "        an_ax.xaxis.set_minor_locator(locator)\n",
    "        an_ax.xaxis.set_minor_formatter(mdates.DateFormatter('%d\\n'))\n",
    "\n",
    "        an_ax.xaxis.grid(True, which=\"minor\")\n",
    "        an_ax.xaxis.grid(True, which=\"major\")\n",
    "\n",
    "\n",
    "        an_ax.xaxis.set_major_locator(mdates.AutoDateLocator())\n",
    "        an_ax.xaxis.set_major_formatter(mdates.DateFormatter('%d\\n'))\n",
    "        \n",
    "\n",
    "        an_ax.yaxis.set_major_formatter(ticker.StrMethodFormatter('{x:.0f}'))\n",
    "        an_ax.yaxis.grid()\n",
    "        an_ax.tick_params(labelsize=22,which=\"minor\")\n",
    "        an_ax.tick_params(labelsize=25,which=\"major\")\n",
    "        \n",
    "        # TODO figure out why we need to autoscale_view\n",
    "        an_ax.autoscale_view()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.margins(x=0)\n",
    "    plt.subplots_adjust(top=0.90)\n",
    "\n",
    "\n",
    "    fig.suptitle('Site {}, Normalized Bi-270min Frequency [%]'.format(selected_area),fontsize=48)\n",
    "#     plt.show()\n",
    "\n",
    "    figDir= Path(visFilePath) / (\"Freq-\"+freq) / regionName \n",
    "    figDir.mkdir(parents=True,exist_ok=True)\n",
    "    figPath= figDir / (\"_\".join([selected_area,str(year)]) +'.'+\"png\")\n",
    "    \n",
    "    fig.savefig(figPath)\n",
    "#     fig.show()\n",
    "#     fig.savefig(\"test\" +'.png')\n",
    "    plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "speechEnv",
   "language": "python",
   "name": "speechenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
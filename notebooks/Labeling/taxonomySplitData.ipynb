{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from ortools.linear_solver import pywraplp\n",
    "\n",
    "from scipy.stats import entropy as kl_div\n",
    "from numpy.linalg import norm\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "# sheetOrg=\"/Users/berk/Desktop/NNA/downloads/Sheet1.csv\"\n",
    "# sheetMine=\"/Users/berk/Desktop/NNA/downloads/Sheet1(1).csv\"\n",
    "\n",
    "resources_folder = ('/scratch/enis/archive/' +\n",
    "                    'forks/cramer2020icassp/resources/')\n",
    "src_path = '/scratch/enis/data/nna/labeling/megan/AudioSamplesPerSite/'\n",
    "megan_labeled_files_info_path = src_path + 'meganLabeledFiles_wlenV1.txt'\n",
    "\n",
    "# csv4megan_excell = (resources_folder + 'Sheet1.csv')\n",
    "csv4megan_excell_clenaed = (resources_folder + 'Sheet1(1).csv')\n",
    "csv4megan_excell = (resources_folder + 'Sheet1.csv')\n",
    "\n",
    "\n",
    "with open(csv4megan_excell_clenaed) as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    reader= list(reader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reader[0]\n",
    "\n",
    "from typing import Dict, Union, Optional, Type\n",
    "from nna import dataimport\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_taxo_code2dataset(megan_data_sheet, audio_dataset):\n",
    "    '''Go through rows of the excell and store taxonomy info into audio_dataset\n",
    "    '''\n",
    "    codest_dict = {}\n",
    "    \n",
    "    for row in megan_data_sheet:\n",
    "        try:\n",
    "            taxonomy_code = dataimport.megan_excell_row2yaml_code(row, None)\n",
    "            site_id=row['Site ID'].strip()\n",
    "            codest_dict.setdefault(taxonomy_code,Counter({}))\n",
    "\n",
    "            codest_dict[taxonomy_code]=codest_dict[taxonomy_code]+Counter({site_id:1})\n",
    "        except:\n",
    "            print(row)\n",
    "    return codest_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [0.6,0.2,0.2]\n",
    "# [0.7,0.15,0.15]\n",
    "# [0.8,0.1,0.1]\n",
    "def getCombinations(total):\n",
    "    combinations=set()\n",
    "    for i in range(100,201,1):\n",
    "        test_val_dist=i/1000\n",
    "        train_dist=1-(test_val_dist*2)\n",
    "        dist=np.array([train_dist,test_val_dist,test_val_dist])\n",
    "        bin_capacities=tuple(np.ceil(total*dist).astype(\"int\"))\n",
    "        combinations.add(bin_capacities)\n",
    "\n",
    "    #add some combinations that are test and valid are bigger so that small number of elements can be handled\n",
    "    combinations2=combinations.copy()\n",
    "    for comb in combinations2:\n",
    "        for rate in [1.2,1.4,1.6,1.8,2.0]:\n",
    "            newComb=(comb[0],np.ceil(comb[1]*rate),np.ceil(comb[1]*rate))\n",
    "            combinations.add(newComb)\n",
    "\n",
    "    return combinations\n",
    "\n",
    "# test\n",
    "total=110\n",
    "# dist = np.array([0.6,0.2,0.2])\n",
    "print(getCombinations(total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def JSD(P, Q):\n",
    "    _P = P / norm(P, ord=1)\n",
    "    _Q = Q / norm(Q, ord=1)\n",
    "    _M = 0.5 * (_P + _Q)\n",
    "    return 0.5 * (kl_div(_P, _M) + kl_div(_Q, _M))\n",
    "\n",
    "def create_data_model(weights,values,bin_capacities):\n",
    "    \"\"\"Create the data for the example.\"\"\"\n",
    "    data = {}\n",
    "    data['weights'] = weights\n",
    "    data['values'] = values\n",
    "    data['items'] = list(range(len(weights)))\n",
    "    data['num_items'] = len(weights)\n",
    "    num_bins = 5\n",
    "    data['bins'] = list(range(3))\n",
    "    total=sum(weights)\n",
    "#     print(total)\n",
    "#     bin_capacities = np.ceil(total*dist).astype(\"int\")\n",
    "    data['bin_capacities'] = bin_capacities\n",
    "    \n",
    "    return data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# [(k,v) for (k,v) in codesDict.items() if sum(v.values())<25]\n",
    "\n",
    "def main(codesDict,dist):\n",
    "    total=0\n",
    "    solutionPerTaxonomy={}\n",
    "    for k in codesDict.keys():\n",
    "        \n",
    "        weights=list(codesDict[k].values())\n",
    "        values = list(codesDict[k].values())\n",
    "        if sum(weights)<10:\n",
    "            print(k)\n",
    "            print(\"Error, too small\", weights)\n",
    "            continue\n",
    "        if len(weights)<3:\n",
    "            print(k)\n",
    "            print(\"Error, number of elements less than 3\", weights)\n",
    "            continue\n",
    "        \n",
    "        total=sum(weights)\n",
    "        \n",
    "\n",
    "        combinations=getCombinations(total,dist)\n",
    "        \n",
    "#         solutionPerCombination=[]\n",
    "        solutionPerTaxonomy.setdefault(k,[])\n",
    "        for bin_capacities in combinations:\n",
    "            \n",
    "            data = create_data_model(weights,values,bin_capacities)\n",
    "            # old version of ortools\n",
    "            # Create the mip solver with the CBC backend.\n",
    "#             solver = pywraplp.Solver.CreateSolver('multiple_knapsack_mip', 'CBC')\n",
    "            # new version of ortools=>8.1\n",
    "            solver = pywraplp.Solver.CreateSolver('SCIP')\n",
    "\n",
    "            # Variables\n",
    "            # x[i, j] = 1 if item i is packed in bin j.\n",
    "            x = {}\n",
    "            for i in data['items']:\n",
    "                for j in data['bins']:\n",
    "                    x[(i, j)] = solver.IntVar(0, 1, 'x_%i_%i' % (i, j))\n",
    "\n",
    "            # Constraints\n",
    "            # Each item can be in at most one bin.\n",
    "            for i in data['items']:\n",
    "                solver.Add(sum(x[i, j] for j in data['bins']) <= 1)\n",
    "            # The amount packed in each bin cannot exceed its capacity.\n",
    "            for j in data['bins']:\n",
    "                solver.Add(\n",
    "                    sum(x[(i, j)] * data['weights'][i]\n",
    "                        for i in data['items']) <= data['bin_capacities'][j])\n",
    "\n",
    "            # Objective\n",
    "            objective = solver.Objective()\n",
    "\n",
    "            for i in data['items']:\n",
    "                for j in data['bins']:\n",
    "                    objective.SetCoefficient(x[(i, j)], data['values'][i])\n",
    "            objective.SetMaximization()\n",
    "\n",
    "            status = solver.Solve()\n",
    "\n",
    "            if status == pywraplp.Solver.OPTIMAL:\n",
    "#                 if objective.Value()/sum(data['bin_capacities'])>0.90:\n",
    "#                     continue\n",
    "\n",
    "                total+=sum(data['weights'])\n",
    "\n",
    "#                 print(codesDict[k])\n",
    "#                 print(\"------------\",k,\"--------------\")\n",
    "#                 print('Total packed value:', objective.Value(),\"/\",sum(data['bin_capacities']))\n",
    "\n",
    "    #             print()\n",
    "                total_weight = 0\n",
    "                solution=[list() for i in range(len(data['bins']))]\n",
    "                for binIndex,j in enumerate(data['bins']):\n",
    "                    bin_weight = 0\n",
    "                    bin_value = 0\n",
    "#                     print('Bin ', j, '\\n')\n",
    "                    for i in data['items']:\n",
    "                        if x[i, j].solution_value() > 0:\n",
    "                            solution[j].append(data['weights'][i])\n",
    "#                             print('Item', i, '- weight:', data['weights'][i], ' value:',\n",
    "#                                   data['values'][i])\n",
    "                            bin_weight += data['weights'][i]\n",
    "                            bin_value += data['values'][i]\n",
    "                            \n",
    "#                     print('Packed bin weight:', bin_weight,\"/\",data['bin_capacities'][binIndex])\n",
    "    #                 print('bin capacity:',)\n",
    "    #                 print('Packed bin value:', bin_value)\n",
    "                    total_weight += bin_weight\n",
    "#                 print('Total packed weight:', total_weight)\n",
    "                solutionPerTaxonomy[k].append((bin_capacities,solution[:]))\n",
    "            else:\n",
    "                print('The problem does not have an optimal solution.')\n",
    "#             print(\"total\",total)\n",
    "    return solutionPerTaxonomy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(codesDict):\n",
    "    expectedDist=[0.6,0.2,0.2]\n",
    "    results=[]\n",
    "    BestSolutionPerTaxonomy={}\n",
    "\n",
    "    for taxoKey in solutionPerTaxonomy:\n",
    "        found=False\n",
    "        total=sum(codesDict[taxoKey].values())\n",
    "\n",
    "        for i in solutionPerTaxonomy[taxoKey]:\n",
    "            if total==sum([sum(m) for m in i[1]]):\n",
    "                found=True\n",
    "        if found is False:\n",
    "            print(codesDict[taxoKey].values())\n",
    "            print(total)\n",
    "\n",
    "    #         for i in solutionPerTaxonomy[taxoKey]:\n",
    "    #             print(i[0],,sum([sum(m) for m in i[1]]))\n",
    "\n",
    "        costPerDist=[]\n",
    "        smallest=999999\n",
    "        bestComp=None\n",
    "        bestDist=None\n",
    "        for i in solutionPerTaxonomy[taxoKey]:\n",
    "    #         print(dist=[sum(m) for m in i[1]])\n",
    "            dist=[sum(m) for m in i[1]]\n",
    "            cost=JSD(expectedDist,dist)\n",
    "            if cost<smallest and total-sum(dist)==0:\n",
    "                smallest=cost\n",
    "                bestComp=i[1]\n",
    "                bestDist=[sum(m) for m in i[1]]\n",
    "\n",
    "        combinedSorted=sorted(list(zip(bestDist,bestComp)),reverse=True)\n",
    "        a,b=[],[]\n",
    "        for m in combinedSorted:\n",
    "            a.append(m[0])\n",
    "            b.append(m[1])\n",
    "        bestDist,bestComp=a,b\n",
    "\n",
    "        results.append([cost,bestDist,bestComp])\n",
    "\n",
    "        BestSolutionPerTaxonomy[taxoKey] = [cost,bestDist,bestComp]\n",
    "    return results,BestSolutionPerTaxonomy\n",
    "\n",
    "    #     if total-sum(bestDist)!=0:\n",
    "    #         print(\"BAD\")\n",
    "    #     print(taxoKey)\n",
    "    #     print(codesDict[taxoKey].values())\n",
    "    #     print(\"total\",total,sum(bestDist),bestDist)\n",
    "\n",
    "    #     print(bestComp)\n",
    "    # #         costPerDist.append()\n",
    "    #     for i in solutionPerTaxonomy[taxoKey]:\n",
    "    #         dist=[sum(m) for m in i[1]]\n",
    "    #         print(dist,sum(dist))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# codesDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BestSolutionPerTaxonomy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func2(codesDict,BestSolutionPerTaxonomy):\n",
    "    solReverse={i:{} for i in codesDict.keys()}\n",
    "    for taxo, counter in codesDict.items():\n",
    "        counter=dict(counter)\n",
    "        for x,y in counter.items():\n",
    "            solReverse[taxo].setdefault(y, []).append(x)\n",
    "\n",
    "    BestSolutionPerTaxonomyLocation={i:None for i in BestSolutionPerTaxonomy.keys()}\n",
    "    for taxo, data in BestSolutionPerTaxonomy.items():\n",
    "    #     print(taxo,data)\n",
    "        comb=data[2]\n",
    "        train,test,val=comb[:]\n",
    "        combLocation=[[] for i in range(len(comb))]\n",
    "        for i,dataSet in enumerate(comb):\n",
    "            for v in dataSet:\n",
    "                location=solReverse[taxo][v].pop()\n",
    "                combLocation[i].append(location)\n",
    "        BestSolutionPerTaxonomyLocation[taxo]=combLocation\n",
    "    return BestSolutionPerTaxonomyLocation\n",
    "    #     print(combLocation)\n",
    "    #     break\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# excellNames2code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def func3(BestSolutionPerTaxonomyLocation,excellNames2code=None,):\n",
    "    # train test valid\n",
    "    # BestSolutionPerTaxonomyLocation\n",
    "    if excellNames2code is None:\n",
    "        excell_names2code = {\n",
    "                'anth': '0.0.0',\n",
    "                'auto': '0.1.0',\n",
    "                'bio': '1.0.0',\n",
    "                'bird': '1.1.0',\n",
    "                'bug': '1.3.0',\n",
    "                'dgs': '1.1.7',\n",
    "                'flare': '0.4.0',\n",
    "                'fox': '1.2.4',\n",
    "                'geo': '2.0.0',\n",
    "                'grouse': '1.1.8',\n",
    "                'loon': '1.1.3',\n",
    "                'mam': '1.2.0',\n",
    "                'plane': '0.2.0',\n",
    "                'ptarm': '1.1.8',\n",
    "                'rain': '2.1.0',\n",
    "                'seab': '1.1.5',\n",
    "                'silence': '3.0.0',\n",
    "                'songbird': '1.1.10',\n",
    "                'unknown': 'X.X.X',\n",
    "                'water': '2.2.0',\n",
    "                'x': 'X.X.X',\n",
    "            }\n",
    "    for yamlCode, data in BestSolutionPerTaxonomyLocation.items():\n",
    "    #     print(yamlCode)\n",
    "        fileCode=yamlCode.replace(\".\",\"-\")\n",
    "        for dataSet in data:\n",
    "            for loc in dataSet:\n",
    "                fileName=(\"_\".join([\"site-\"+str(loc),fileCode,\"original.h5\"]))\n",
    "                pathFile=\"./resources/myDatasets/megan/\"+fileName\n",
    "#                 print(pathFile)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# birdvox-cls-test\n",
    "# birdvox-cls-train\n",
    "# birdvox-cls-valid\n",
    "# load files with librosa, sample to \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "codest_dict2 = add_taxo_code2dataset(reader,[])\n",
    "total2=110\n",
    "dist2 = np.array([0.6,0.2,0.2])\n",
    "#test\n",
    "# np.ceil(total*dist).astype(\"int\")\n",
    "solutionPerTaxonomy=main(codest_dict2,dist2)\n",
    "results2,BestSolutionPerTaxonomy2 =func(codest_dict2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "results2=sorted(results2,reverse=True)\n",
    "[i[1] for i in results2]\n",
    "# len(results),len(codesDict.keys())\n",
    "\n",
    "BestSolutionPerTaxonomyLocation2 = func2(codest_dict2,BestSolutionPerTaxonomy2)\n",
    "# BestSolutionPerTaxonomyLocation\n",
    "func3(BestSolutionPerTaxonomyLocation2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BestSolutionPerTaxonomyLocation2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To add a new cell, type '# %%'\n",
    "# To add a new markdown cell, type '# %% [markdown]'\n",
    "# %%\n",
    "from __future__ import print_function\n",
    "from ortools.linear_solver import pywraplp\n",
    "\n",
    "from scipy.stats import entropy as kl_div\n",
    "from numpy.linalg import norm\n",
    "import numpy as np\n",
    "\n",
    "# %%\n",
    "import csv\n",
    "from collections import Counter\n",
    "\n",
    "# sheetOrg='/Users/berk/Desktop/NNA/downloads/Sheet1.csv'\n",
    "# sheetMine='/Users/berk/Desktop/NNA/downloads/Sheet1(1).csv'\n",
    "\n",
    "resources_folder = ('/scratch/enis/archive/' +\n",
    "                    'forks/cramer2020icassp/resources/')\n",
    "src_path = '/scratch/enis/data/nna/labeling/megan/AudioSamplesPerSite/'\n",
    "megan_labeled_files_info_path = src_path + 'meganLabeledFiles_wlenV1.txt'\n",
    "\n",
    "# csv4megan_excell = (resources_folder + 'Sheet1.csv')\n",
    "csv4megan_excell_clenaed = (resources_folder + 'Sheet1(1).csv')\n",
    "csv4megan_excell = (resources_folder + 'Sheet1.csv')\n",
    "\n",
    "with open(csv4megan_excell_clenaed) as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    reader = list(reader)\n",
    "\n",
    "# %%\n",
    "# reader[0]\n",
    "\n",
    "# from typing import Dict, Union, Optional, Type\n",
    "from nna import dataimport\n",
    "\n",
    "\n",
    "# %%\n",
    "def add_taxo_code2dataset(megan_data_sheet):\n",
    "    '''Create Counter for each taxonomy from excell sheet\n",
    "\n",
    "        Returns:\n",
    "            {'1.1.1':Counter('22':5,'11':10...),\n",
    "            '0.1.1':Counter('5':5,'6':10...)}\n",
    "\n",
    "        todo remove try except\n",
    "    '''\n",
    "    codest_dict = {}\n",
    "    for row in megan_data_sheet:\n",
    "        try:\n",
    "            taxonomy_code = dataimport.megan_excell_row2yaml_code(row, None)\n",
    "            site_id = row['Site ID'].strip()\n",
    "            codest_dict.setdefault(taxonomy_code, Counter({}))\n",
    "\n",
    "            codest_dict[taxonomy_code] = codest_dict[taxonomy_code] + Counter(\n",
    "                {site_id: 1})\n",
    "        except:\n",
    "            print(row)\n",
    "    return codest_dict\n",
    "\n",
    "\n",
    "# %%\n",
    "# [0.6,0.2,0.2]\n",
    "# [0.7,0.15,0.15]\n",
    "# [0.8,0.1,0.1]\n",
    "def approx_split_combinations(total):\n",
    "    '''calculate posssible combinations that sums to  ~total (aproximate). \n",
    "\n",
    "        Distribution assumptions:\n",
    "            test~=valid, \n",
    "            0.8~>train~>0.6\n",
    "            0.4~>test,valid~>0.2\n",
    "\n",
    "    example:\n",
    "        total=110\n",
    "        print(getCombinations(total))\n",
    "            {(70, 42.0, 42.0), (71, 20, 20), (88, 15.0, 15.0), ...}\n",
    "    '''\n",
    "    combinations = set()\n",
    "    for i in range(100, 201, 1):\n",
    "        test_val_dist = i / 1000\n",
    "        train_dist = 1 - (test_val_dist * 2)\n",
    "        dist = np.array([train_dist, test_val_dist, test_val_dist])\n",
    "        bin_capacities = tuple(np.ceil(total * dist).astype('int')) # type: ignore\n",
    "        combinations.add(bin_capacities)\n",
    "\n",
    "    #add some combinations that are test and valid\n",
    "    # are bigger so that small number of elements can be handled\n",
    "    combinations2 = combinations.copy()\n",
    "    for comb in combinations2:\n",
    "        for rate in [1.2, 1.4, 1.6, 1.8, 2.0]:\n",
    "            newComb = (comb[0], np.ceil(comb[1] * rate),\n",
    "                       np.ceil(comb[1] * rate))\n",
    "            combinations.add(newComb)\n",
    "\n",
    "    return combinations\n",
    "\n",
    "\n",
    "# test\n",
    "# print(getCombinations(total))\n",
    "\n",
    "# %%\n",
    "\n",
    "\n",
    "def JSD(P, Q):\n",
    "    '''\n",
    "\n",
    "        todo: add reference and docs\n",
    "    '''\n",
    "    _P = P / norm(P, ord=1)\n",
    "    _Q = Q / norm(Q, ord=1)\n",
    "    _M = 0.5 * (_P + _Q)\n",
    "    return 0.5 * (kl_div(_P, _M) + kl_div(_Q, _M))\n",
    "\n",
    "\n",
    "def create_data_model(weights, values, bin_capacities):\n",
    "    '''Create variables for knapsack problem setup.\n",
    "\n",
    "    '''\n",
    "    data = {}\n",
    "    data['weights'] = weights\n",
    "    data['values'] = values\n",
    "    data['items'] = list(range(len(weights)))\n",
    "    data['num_items'] = len(weights)\n",
    "    data['bins'] = list(range(len(bin_capacities)))\n",
    "    data['bin_capacities'] = bin_capacities\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "# %%\n",
    "\n",
    "\n",
    "def multiple_knapsack_solve(codes_dict):\n",
    "    '''Solves a knapsack problems for each key of codes_dict.\n",
    "\n",
    "        codes_dict's keys are names and values are Counters of samples on\n",
    "            each location. Items of a counter are items in the knapsack problem.\n",
    "            We try to place each location into one of the train,test,valid bins.\n",
    "\n",
    "            Expected distribution is calculated by approx_split_combinations.\n",
    "\n",
    "        Returns: a dict solution_per_taxonomy\n",
    "            keys are same with codes_dict and values are possible solutions to\n",
    "            the knapsack problem. \n",
    "\n",
    "    '''\n",
    "    total = 0\n",
    "    solution_per_taxonomy = {}\n",
    "    for k in codes_dict.keys():\n",
    "\n",
    "        weights = list(codes_dict[k].values())\n",
    "        values = list(codes_dict[k].values())\n",
    "        if sum(weights) < 10:\n",
    "            print(k)\n",
    "            print('Error, too small', weights)\n",
    "            continue\n",
    "        if len(weights) < 3:\n",
    "            print(k)\n",
    "            print('Error, number of elements less than 3', weights)\n",
    "            continue\n",
    "\n",
    "        total = sum(weights)\n",
    "\n",
    "        combinations = approx_split_combinations(total)\n",
    "\n",
    "        #         solutionPerCombination=[]\n",
    "        solution_per_taxonomy.setdefault(k, [])\n",
    "        for bin_capacities in combinations:\n",
    "\n",
    "            data = create_data_model(weights, values, bin_capacities)\n",
    "            # old version of ortools\n",
    "            # Create the mip solver with the CBC backend.\n",
    "            #             solver = pywraplp.Solver.CreateSolver('multiple_knapsack_mip', 'CBC')\n",
    "            # new version of ortools=>8.1\n",
    "            solver = pywraplp.Solver.CreateSolver('SCIP') # type: ignore\n",
    "\n",
    "            # Variables\n",
    "            # x[i, j] = 1 if item i is packed in bin j.\n",
    "            x = {}\n",
    "            for i in data['items']:\n",
    "                for j in data['bins']:\n",
    "                    x[(i, j)] = solver.IntVar(0, 1, 'x_%i_%i' % (i, j))\n",
    "\n",
    "            # Constraints\n",
    "            # Each item can be in at most one bin.\n",
    "            for i in data['items']:\n",
    "                solver.Add(sum(x[i, j] for j in data['bins']) <= 1)\n",
    "            # The amount packed in each bin cannot exceed its capacity.\n",
    "            for j in data['bins']:\n",
    "                solver.Add(\n",
    "                    sum(x[(i, j)] * data['weights'][i]\n",
    "                        for i in data['items']) <= data['bin_capacities'][j])\n",
    "\n",
    "            # Objective\n",
    "            objective = solver.Objective()\n",
    "\n",
    "            for i in data['items']:\n",
    "                for j in data['bins']:\n",
    "                    objective.SetCoefficient(x[(i, j)], data['values'][i])\n",
    "            objective.SetMaximization()\n",
    "\n",
    "            status = solver.Solve()\n",
    "\n",
    "            if status == pywraplp.Solver.OPTIMAL:\n",
    "                #                 if objective.Value()/sum(data['bin_capacities'])>0.90:\n",
    "                #                     continue\n",
    "\n",
    "                total += sum(data['weights'])\n",
    "\n",
    "                #                 print(codesDict[k])\n",
    "                #                 print('------------',k,'--------------')\n",
    "                #                 print('Total packed value:', objective.Value(),'/',sum(data['bin_capacities']))\n",
    "\n",
    "                #             print()\n",
    "                total_weight = 0\n",
    "                solution = [list() for i in range(len(data['bins']))]\n",
    "                for binIndex, j in enumerate(data['bins']):\n",
    "                    bin_weight = 0\n",
    "                    bin_value = 0\n",
    "                    #                     print('Bin ', j, '\\n')\n",
    "                    for i in data['items']:\n",
    "                        if x[i, j].solution_value() > 0:\n",
    "                            solution[j].append(data['weights'][i])\n",
    "                            #                             print('Item', i, '- weight:', data['weights'][i], ' value:',\n",
    "                            #                                   data['values'][i])\n",
    "                            bin_weight += data['weights'][i]\n",
    "                            bin_value += data['values'][i]\n",
    "\n",
    "#                     print('Packed bin weight:', bin_weight,'/',data['bin_capacities'][binIndex])\n",
    "#                 print('bin capacity:',)\n",
    "#                 print('Packed bin value:', bin_value)\n",
    "                    total_weight += bin_weight\n",
    "#                 print('Total packed weight:', total_weight)\n",
    "                solution_per_taxonomy[k].append((bin_capacities, solution[:]))\n",
    "            else:\n",
    "                print('The problem does not have an optimal solution.')\n",
    "#             print('total',total)\n",
    "    return solution_per_taxonomy\n",
    "\n",
    "\n",
    "# %%\n",
    "\n",
    "\n",
    "# %%\n",
    "def find_best_solution(codes_dict):\n",
    "    '''\n",
    "\n",
    "    Return:\n",
    "        {'1.0.0': [0.0026258626328006605,\n",
    "            [29, 9, 9],\n",
    "            [[6, 11, 5, 7], [5, 3, 1], [1, 2, 1, 1, 1, 1, 1, 1]]],\n",
    "            '3.0.0': [0.01975404831728412,\n",
    "            [27, 10, 9],\n",
    "            [[4, 4, 3, 5, 2, 5, 2, 2], [5, 2, 3], [5, 4]]], ...}\n",
    "\n",
    "    '''\n",
    "    expectedDist = [0.6, 0.2, 0.2]\n",
    "    results = []\n",
    "    best_solution_per_taxonomy = {}\n",
    "\n",
    "    for taxoKey in solutionPerTaxonomy:\n",
    "        found = False\n",
    "        total = sum(codes_dict[taxoKey].values())\n",
    "\n",
    "        for a_solution in solutionPerTaxonomy[taxoKey]:\n",
    "            if total == sum([sum(m) for m in a_solution[1]]):\n",
    "                found = True\n",
    "        if found is False:\n",
    "            print(codes_dict[taxoKey].values())\n",
    "            print(total)\n",
    "\n",
    "    #         for i in solutionPerTaxonomy[taxoKey]:\n",
    "    #             print(i[0],,sum([sum(m) for m in i[1]]))\n",
    "\n",
    "        smallest_cost = 999999\n",
    "        best_comb = None\n",
    "        best_dist = None\n",
    "        for a_solution in solutionPerTaxonomy[taxoKey]:\n",
    "            #         print(dist=[sum(m) for m in i[1]])\n",
    "            dist = [sum(m) for m in a_solution[1]]\n",
    "            cost = JSD(expectedDist, dist)\n",
    "            if cost < smallest_cost and total - sum(dist) == 0:\n",
    "                smallest_cost = cost\n",
    "                best_comb = a_solution[1]\n",
    "                best_dist = [sum(m) for m in a_solution[1]]\n",
    "\n",
    "        # sort best_dist and best_comb\n",
    "        combined_sorted = sorted(list(zip(best_dist, best_comb)), reverse=True) # type: ignore\n",
    "        a, b = [], []\n",
    "        for m in combined_sorted:\n",
    "            a.append(m[0])\n",
    "            b.append(m[1])\n",
    "        best_dist, best_comb = a, b\n",
    "\n",
    "        results.append([smallest_cost, best_dist, best_comb])\n",
    "\n",
    "        best_solution_per_taxonomy[taxoKey] = [\n",
    "            smallest_cost, best_dist, best_comb\n",
    "        ]\n",
    "    return results, best_solution_per_taxonomy\n",
    "\n",
    "\n",
    "# %%\n",
    "def knapsack_index2location_name(codesDict, best_solution_per_taxonomy):\n",
    "    '''Replace indexes of locations with location names. \n",
    "        \n",
    "        returns:\n",
    "            {'1.0.0': [['44', '46', '17', '14'],\n",
    "                ['11', '34', '27'],\n",
    "                ['31', '50', '18', '12', '30', '39', '48', '45']],\n",
    "                '3.0.0': [['40', '20', '14', '17', '13', '36', '25', '33'],\n",
    "                ['18', '38', '39'],\n",
    "                ['32', '45']],\n",
    "                ...}\n",
    "    '''\n",
    "    solReverse = {i: {} for i in codesDict.keys()}\n",
    "    for taxo, counter in codesDict.items():\n",
    "        counter = dict(counter)\n",
    "        for x, y in counter.items():\n",
    "            solReverse[taxo].setdefault(y, []).append(x)\n",
    "\n",
    "    best_solution_per_taxonomy_by_location = {\n",
    "        i: None for i in best_solution_per_taxonomy.keys()\n",
    "    }\n",
    "    for taxo, data in best_solution_per_taxonomy.items():\n",
    "        #     print(taxo,data)\n",
    "        comb = data[2]\n",
    "        train, test, val = comb[:]\n",
    "        combLocation = [[] for i in range(len(comb))]\n",
    "        for i, dataSet in enumerate(comb):\n",
    "            for v in dataSet:\n",
    "                location = solReverse[taxo][v].pop()\n",
    "                combLocation[i].append(location)\n",
    "        best_solution_per_taxonomy_by_location[taxo] = combLocation # type: ignore\n",
    "    return best_solution_per_taxonomy_by_location\n",
    "\n",
    "# %%\n",
    "# excellNames2code\n",
    "\n",
    "# %%\n",
    "\n",
    "\n",
    "def results2file_names_like_cramer(\n",
    "    BestSolutionPerTaxonomyLocation,\n",
    "    excellNames2code=None,\n",
    "):\n",
    "    # train test valid\n",
    "    # BestSolutionPerTaxonomyLocation\n",
    "    if excellNames2code is None:\n",
    "        excell_names2code = {\n",
    "            'anth': '0.0.0',\n",
    "            'auto': '0.1.0',\n",
    "            'bio': '1.0.0',\n",
    "            'bird': '1.1.0',\n",
    "            'bug': '1.3.0',\n",
    "            'dgs': '1.1.7',\n",
    "            'flare': '0.4.0',\n",
    "            'fox': '1.2.4',\n",
    "            'geo': '2.0.0',\n",
    "            'grouse': '1.1.8',\n",
    "            'loon': '1.1.3',\n",
    "            'mam': '1.2.0',\n",
    "            'plane': '0.2.0',\n",
    "            'ptarm': '1.1.8',\n",
    "            'rain': '2.1.0',\n",
    "            'seab': '1.1.5',\n",
    "            'silence': '3.0.0',\n",
    "            'songbird': '1.1.10',\n",
    "            'unknown': 'X.X.X',\n",
    "            'water': '2.2.0',\n",
    "            'x': 'X.X.X',\n",
    "        }\n",
    "    for yamlCode, data in BestSolutionPerTaxonomyLocation.items():\n",
    "        #     print(yamlCode)\n",
    "        fileCode = yamlCode.replace('.', '-')\n",
    "        for dataSet in data:\n",
    "            for loc in dataSet:\n",
    "                fileName = ('_'.join(\n",
    "                    ['site-' + str(loc), fileCode, 'original.h5']))\n",
    "                pathFile = './resources/myDatasets/megan/' + fileName\n",
    "\n",
    "\n",
    "#                 print(pathFile)\n",
    "\n",
    "# %%\n",
    "# birdvox-cls-test\n",
    "# birdvox-cls-train\n",
    "# birdvox-cls-valid\n",
    "# load files with librosa, sample to\n",
    "\n",
    "# %%\n",
    "\n",
    "codest_dict2 = add_taxo_code2dataset(reader)\n",
    "total2 = 110\n",
    "dist2 = np.array([0.6, 0.2, 0.2])\n",
    "#test\n",
    "# np.ceil(total*dist).astype('int')\n",
    "solutionPerTaxonomy = multiple_knapsack_solve(codest_dict2)\n",
    "results2, BestSolutionPerTaxonomy2 = find_best_solution(codest_dict2)\n",
    "\n",
    "# %%\n",
    "\n",
    "results2 = sorted(results2, reverse=True)\n",
    "[i[1] for i in results2]\n",
    "# len(results),len(codesDict.keys())\n",
    "\n",
    "BestSolutionPerTaxonomyLocation2 = knapsack_index2location_name(codest_dict2, BestSolutionPerTaxonomy2)\n",
    "# BestSolutionPerTaxonomyLocation\n",
    "results2file_names_like_cramer(BestSolutionPerTaxonomyLocation2)\n",
    "\n",
    "# %%\n",
    "\n",
    "# %%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BestSolutionPerTaxonomyLocation2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "soundenv3",
   "language": "python",
   "name": "soundenv3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from nna.fileUtils import list_files,save_to_csv\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/enis/projects/similarSoundsApp\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('/home/enis/projects/similarSoundsApp/'))\n",
    "print(module_path)\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "\n",
    "from nnSearch import loadOnDisk,buildOnDisk\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexIDname = \"photoEmbeddings_V2_hamming\"\n",
    "\n",
    "outputFolder = Path(\"/scratch/enis/data/nna/realMerged/\")\n",
    "metadataCSV = outputFolder / (indexIDname+\".csv\")\n",
    "fn = outputFolder / (indexIDname+\".ann\")\n",
    "\n",
    "# vector size\n",
    "f=128\n",
    "  # Length of item vector that will be indexed\n",
    "distance='hamming'\n",
    "#prepares annoy to build the index in the specified file instead of RAM \n",
    "#(execute before adding items, no need to save after build)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Load embeddings to memory\n",
    "#### 1.0) find images so we can just load embeddings from these locations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load datasets\n",
    "realFolderPath=\"/scratch/enis/data/nna/real/\"\n",
    "\n",
    "\n",
    "# Look where the images are so we index only these locations\n",
    "pathDatasets = \"/home/enis/projects/similarSoundsApp/resources/TimeLapse_Databases/\"\n",
    "datasetList=[[m for m in i.glob(\"*\") if \".ddb\" in str(m) ][0] for i in (Path(pathDatasets).glob(\"*\"))]\n",
    "# locations with only images\n",
    "labeledImgLocations = [m.parent.stem for m in datasetList]\n",
    "# all existing locations\n",
    "region_locId=[[m for m in i.glob(\"*\") if m.is_dir() ] for i in (Path(realFolderPath).glob(\"*\")) if i.is_dir()]\n",
    "\n",
    "#keep locations only with images\n",
    "allLocations=[]\n",
    "for m in region_locId:\n",
    "    for n in m:\n",
    "        if n.stem in labeledImgLocations:\n",
    "            allLocations.append(n)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 find embeddings in the given locations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "filenamePattern=\"*_embeddings*.npy\"\n",
    "all_embeddings=[]\n",
    "for loc in allLocations:\n",
    "    # find all files\n",
    "    search_path=loc\n",
    "    all_embeddings+=list_files(str(search_path),filename=filenamePattern)\n",
    "\n",
    "print(len(all_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of embeddings 11235 for photoEmbeddings_V1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1) save metadata, embedding file names and sizes as a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedSizes=[]\n",
    "for i,embed_file in enumerate(all_embeddings):\n",
    "    embed=np.load(embed_file)\n",
    "    embedSizes.append(embed.shape[0])\n",
    "\n",
    "metadata = list(zip(all_embeddings,embedSizes))\n",
    "save_to_csv(metadataCSV,metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2) Create and save annoy index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current index: 0\n",
      "Current index: 1000\n",
      "Current index: 2000\n",
      "Current index: 3000\n",
      "Current index: 4000\n",
      "Current index: 5000\n",
      "Current index: 6000\n",
      "Current index: 7000\n",
      "Current index: 8000\n",
      "Current index: 9000\n",
      "Current index: 10000\n",
      "Current index: 11000\n"
     ]
    }
   ],
   "source": [
    "p=Path(fn)\n",
    "if p.exists():\n",
    "    print(p,\"EXISTS\")\n",
    "    t = loadOnDisk(fn,f,distance=distance)\n",
    "else:\n",
    "    t = buildOnDisk(fn,f,distance=distance,seed=42,treeN=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) create functions caches depending on the Index and Metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1) npy2originalFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NNA folder\n",
    "nnaProjectDataFolder = Path(\"/home/enis/projects/nna/data/\")\n",
    "\n",
    "# name and folder of the cached function  file\n",
    "npy2originalFileCacheFileName = \"npy2originalFileCache_\"+indexIDname+\".pkl\"\n",
    "pklPath = nnaProjectDataFolder / \"funcCache\" / npy2originalFileCacheFileName\n",
    "\n",
    "\n",
    "# parameters for the npy2originalFile\n",
    "file_properties_df_FilePath = nnaProjectDataFolder / \"allFields_dataV3.pkl\"\n",
    "\n",
    "embeddataPath=\"/scratch/enis/data/nna/real/\"\n",
    "outputPath=embeddataPath\n",
    "inputPath=\"/tank/data/nna/real/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nna.fileUtils import npy2originalFile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters ,inputPath,outputPath,file_properties_df\n",
    "\n",
    "npy2originalFileCache={}\n",
    "for i,meta in enumerate(metadata):\n",
    "    orgFilePath = npy2originalFile(Path(meta[0]),inputPath,outputPath,\n",
    "                                               file_properties_df,subDirectoryAddon=None,\n",
    "                                               fileNameAddon=None,debug=0)\n",
    "    npy2originalFileCache[meta[0]] = orgFilePath\n",
    "    if i%1000==0:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npy2originalFileCache[\"params\"] = [None,inputPath,outputPath,str(file_properties_df_FilePath),None,None,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "\n",
    "with open(pklPath, 'wb') as handle:\n",
    "    pickle.dump(npy2originalFileCache, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    \n",
    "with open(pklPath, 'rb') as handle:\n",
    "    b = pickle.load(handle)\n",
    "\n",
    "print (npy2originalFileCache == b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "speechEnv",
   "language": "python",
   "name": "speechenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/enis/projects/nna/src\n"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "# import sys\n",
    "\n",
    "# module_path = os.path.abspath(os.path.join('../src'))\n",
    "# print(module_path)\n",
    "# if module_path not in sys.pathF:\n",
    "#     sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nna.fileUtils import list_files\n",
    "\n",
    "from pathlib import Path\n",
    "from nna.pre_process_func import read_queue\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# originally from ../src/notes.md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* start with screen\n",
    "* get GPU Memory so I can use it later\n",
    "* one file file at a time (~2 gb)\n",
    "  * I will divide files to 1 hour slices\n",
    "  * I will use 25 cpus so 25 processes\n",
    "  * embeddings file name will be filename_embeddings.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) create a text file with paths to input audio files (one full path per line)\n",
    " \n",
    "* `find /tank/data/nna/real/ -iname \"*flac\" > flacfiles.txt`\n",
    "* or following code, list_files function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "datafiles_path=\"/home/enis/projects/nna/data/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_path=\"/tank/data/nna/real/\"\n",
    "ignore_folders=[\"/tank/data/nna/real/ivvavik/\",\"/tank/data/nna/real/stinchcomb/dups/\",\"/tank/data/nna/real/stinchcomb/excerpts/\"]\n",
    "\n",
    "files_path_list=list_files(search_path,ignore_folders)\n",
    "files_path_list=list(set(files_path_list))\n",
    "\n",
    "datafiles_path=\"/home/enis/projects/nna/data/\"\n",
    "input_filename = \"ExperimentRunV5.txt\"\n",
    "\n",
    "thepath=datafiles_path+input_filename\n",
    "with open(thepath,\"w\") as myfile:\n",
    "    myfile.writelines(\"\\n\".join(files_path_list))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) update input, output directory, input list,  in /nna/src/params.py\n",
    "\n",
    "3-1) sync files \n",
    "\n",
    "`rsync -av --recursive --update ./ enis@crescent:/home/enis/projects/nna/ `\n",
    "\n",
    "3-2) run codes\n",
    "```bash\n",
    "python pre_process.py &>> job_logs/logs.txt; python slack_message.py -t \"cpu job ended\" -f job_logs/logs.txt &\n",
    "\n",
    "python watch_VGGish.py &>> job_logs/logs.txt; python slack_message.py -t \"gpu job stopped\" & \n",
    "```\n",
    "4) to re-run, update the code and remove temporary flac files\n",
    "\n",
    "`rsync -av --recursive --update /Users/berk/Documents/workspace/speech_audio_understanding/src/ enis@crescent:/home/enis/projects/nna/`\n",
    "\n",
    "`find /scratch/enis/data/nna/real/ -iname \"*flac\"  -delete`\n",
    "`find /scratch/enis/data/nna/real/ -path \"*/*_segments*/*\" -delete`\n",
    "`find /scratch/enis/data/nna/real/ -name \"*_segments\" -type d -delete`\n",
    "\n",
    "also remove `job_logs/pre_processing_queue.csv` if jobs left unfinished\n",
    "\n",
    "5) tracking progress \n",
    "```bash\n",
    "cat job_logs/pre_processing_queue.csv | wc -l; cat job_logs/pre_processed_queue.csv | wc -l; cat job_logs/VGGISH_processing_queue.csv | wc -l; cat job_logs/vggish_embeddings_queue.csv | wc -l; \n",
    "du -hs /scratch/enis/data/\n",
    "```\n",
    "6) backup\n",
    "```bash\n",
    "tar cf - /scratch/enis/data/nna/backup/NUI_DATA/ -P | pv -s $(du -sb /scratch/enis/data/nna/backup/NUI_DATA/ | awk '{print $1}') | gzip > embeddings_backup.tar.gz\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1=read_queue(\"../src/job_logs/pre_processing_queue.csv\")\n",
    "q2=read_queue(\"../src/job_logs/pre_processed_queue.csv\")\n",
    "q3=read_queue(\"../src/job_logs/VGGISH_processing_queue.csv\")\n",
    "q4=read_queue(\"../src/job_logs/vggish_embeddings_queue.csv\")\n",
    "q5=read_queue(\"../src/job_logs/Audioset_processing_queue.csv\")\n",
    "q6=read_queue(\"../src/job_logs/Audioset_output_queue.csv\")\n",
    "\n",
    "CABLE=read_queue(\"../src/job_logs/_CABLE_output_queue.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1set=set([\"/\".join(i.split(\"/\")[-1:])[:-5] for i in q1])\n",
    "q2set=set([\"/\".join(i.split(\"/\")[-2:-1])[:-13] for i in q2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'SINP-05_20190709_090000', 'S4A10252_20190529_171602', 'S4A10259_20190514_183000'}\n"
     ]
    }
   ],
   "source": [
    "print(q1set.difference(q2set))\n",
    "print(q2set.difference(q1set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# files without results\n",
    "\n",
    "from visUtils import standardPathStyle\n",
    "import pandas as pd\n",
    "\n",
    "file_properties_df_ST=pd.read_pickle(\"../data/stinchcomb_dataV1.pkl\")\n",
    "file_properties_df=pd.read_pickle(\"../data/realdata_v2No_stinchcomb.pkl\")\n",
    "subDirectoryAddon=\"_vgg\"\n",
    "# subDirectoryAddon=\"_CABLE\"\n",
    "# subDirectoryAddon=\"_CABLE\"\n",
    "subDirectoryAddons=[\"_CABLE\",\"_RUNNINGWATER\",\"_INSECT\", \"_RAIN\", \"_WATERBIRD\", \"_WIND\", \"_SONGBIRD\", \"_AIRCRAFT\"]\n",
    "notDoneFilesDict={}\n",
    "for subDirectoryAddon in subDirectoryAddons:\n",
    "    print(subDirectoryAddon)\n",
    "    notDoneFiles=[]\n",
    "    \n",
    "    for afile,row in file_properties_df.iterrows():\n",
    "        checkFile=standardPathStyle(\"/scratch/enis/data/nna/real/\",row,subDirectoryAddon=subDirectoryAddon,fileNameAddon=\"\")\n",
    "        if not checkFile.exists():\n",
    "#             print(afile)\n",
    "            notDoneFiles.append(afile)\n",
    "\n",
    "    for afile,row in file_properties_df_ST.iterrows():\n",
    "        checkFile=standardPathStyle(\"/scratch/enis/data/nna/real/\",row,subDirectoryAddon=subDirectoryAddon,fileNameAddon=\"\")\n",
    "        if not checkFile.exists():\n",
    "    #         print(afile)\n",
    "#             print(checkFile)\n",
    "            notDoneFiles.append(afile)\n",
    "    notDoneFiles.sort()\n",
    "    notDoneFiles=[str(i) for i in notDoneFiles]\n",
    "    notDoneFilesDict[subDirectoryAddon]=notDoneFiles[:]\n",
    "\n",
    "    \n",
    "\n",
    "# input_filename = \"ExperimentRunV6.txt\"\n",
    "\n",
    "# thepath=datafiles_path+input_filename\n",
    "# with open(thepath,\"w\") as myfile:\n",
    "#     myfile.writelines(\"\\n\".join(notDoneFiles))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FIND missing prediction results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ivvavik 27852\n",
      "anwr 8509\n",
      "prudhoe 8228\n",
      "stinchcomb 19376\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "inputCSV=[]\n",
    "for i in [\"ivvavik\",\"anwr\",  \"prudhoe\" , \"stinchcomb\"]:\n",
    "    x=glob.glob( f\"/scratch/enis/data/nna/real/{i}/**/*_rawembeddings*.npy\",recursive=True)\n",
    "    print(i,len(x))\n",
    "    inputCSV.extend(x)\n",
    "\n",
    "\n",
    "save_to_csv(\"../src/job_logs/inferenceRun3.csv\",[[str(afile)] for afile in inputCSV])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _preprocessed.npy\n",
    "search_path=\"/tank/data/nna/real/\"\n",
    "ignore_folders=[\"/tank/data/nna/real/ivvavik/\",\"/tank/data/nna/real/stinchcomb/dups/\",\"/tank/data/nna/real/stinchcomb/excerpts/\"]\n",
    "\n",
    "files_path_list=list_files(search_path,ignore_folders)\n",
    "files_path_list=list(set(files_path_list))\n",
    "\n",
    "datafiles_path=\"/home/enis/projects/nna/data/\"\n",
    "input_filename = \"ExperimentRunV5.txt\"\n",
    "\n",
    "thepath=datafiles_path+input_filename\n",
    "with open(thepath,\"w\") as myfile:\n",
    "    myfile.writelines(\"\\n\".join(files_path_list))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "inputCsvFile=\"./job_logs/inferenceRun3.csv\"\n",
    "outputLogs=\"job_logs/MLlogs.txt\"\n",
    "i=0\n",
    "python3 watchMLInference.py --inputCsv $inputCsvFile --modelPath \"./assets/sklearnModels/Cable_Gaussian Process_Raw_Concat_2020-03-02--14-06.joblib\"  --modelName \"_CABLE\" &>> $outputLogs & \n",
    "pids[i]=$! && ((i=i+1));\n",
    "python3 watchMLInference.py --inputCsv $inputCsvFile --modelPath \"./assets/sklearnModels/Running Water_Neural Net_Raw_Concat_2020-03-02--14-06.joblib\"  --modelName \"_RUNNINGWATER\" &>> $outputLogs &\n",
    "pids[i]=$! && ((i=i+1));\n",
    "python3 watchMLInference.py --inputCsv $inputCsvFile --modelPath \"./assets/sklearnModels/Insect_Linear SVM_Raw_Concat_2020-03-02--14-06.joblib\"  --modelName \"_INSECT\" &>> $outputLogs & \n",
    "pids[i]=$! && ((i=i+1));\n",
    "python3 watchMLInference.py --inputCsv $inputCsvFile --modelPath \"./assets/sklearnModels/Rain_Gaussian Process_Raw_Concat_2020-03-02--14-06.joblib\"  --modelName \"_RAIN\" &>> $outputLogs &\n",
    "pids[i]=$! && ((i=i+1));\n",
    "python3 watchMLInference.py --inputCsv $inputCsvFile --modelPath \"./assets/sklearnModels/Water Bird_RBF SVM_Raw_many2one_2020-03-02--14-06.joblib\"  --modelName \"_WATERBIRD\" &>> $outputLogs &\n",
    "pids[i]=$! && ((i=i+1));\n",
    "python3 watchMLInference.py --inputCsv $inputCsvFile --modelPath \"./assets/sklearnModels/Wind_Neural Net_Raw_many2one_2020-03-02--14-06.joblib\"  --modelName \"_WIND\" &>> $outputLogs &\n",
    "pids[i]=$! && ((i=i+1));\n",
    "python3 watchMLInference.py --inputCsv $inputCsvFile --modelPath \"./assets/sklearnModels/Songbird_Neural Net_Raw_many2one_2020-03-02--14-06.joblib\"  --modelName \"_SONGBIRD\" &>> $outputLogs &\n",
    "pids[i]=$! && ((i=i+1));\n",
    "python3 watchMLInference.py --inputCsv $inputCsvFile --modelPath \"./assets/sklearnModels/Aircraft_AdaBoost_Raw_Average_2020-03-02--14-06.joblib\"  --modelName \"_AIRCRAFT\" &>> $outputLogs &\n",
    "pids[i]=$! && ((i=i+1));\n",
    "for pid in ${pids[*]}; do\n",
    "    wait $pid\n",
    "done;\n",
    "python3 slack_message.py -t \"MLInference job ended\" -f $outputLogs &\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_CABLE\n",
      "0\n",
      "_RUNNINGWATER\n",
      "0\n",
      "_INSECT\n",
      "0\n",
      "_RAIN\n",
      "0\n",
      "_WATERBIRD\n",
      "0\n",
      "_WIND\n",
      "0\n",
      "_SONGBIRD\n",
      "0\n",
      "_AIRCRAFT\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# set(inputCSV)\n",
    "import time\n",
    "logsPath=\"../src/job_logs/\"\n",
    "inputCSVset=set(inputCSV)\n",
    "for subDirectoryAddon in subDirectoryAddons:\n",
    "    csvFile=logsPath+subDirectoryAddon+\"_output_queue.csv\"\n",
    "    print(subDirectoryAddon)\n",
    "    csvFileSet=set(read_queue(csvFile))\n",
    "    print(len(inputCSVset.difference(csvFileSet)))\n",
    "#     for i in inputCSVset.difference(csvFileSet):\n",
    "#         print(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "speechEnv",
   "language": "python",
   "name": "speechenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
